# Tools for Describing the Relationship Between Two Quantitative Variables {#sec-correlation}

## Introduction to Bivariate Data[^correlation-1]

[^correlation-1]: This section is adapted from Rudy Guerra and David M. Lane. “Introduction to Bivariate Data.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/describing_bivariate_data/intro.html>

Measures of central tendency, variability, and spread summarize a single variable by providing important information about its distribution. Often, more than one variable is collected on each individual. For example, in large health studies of populations it is common to obtain variables such as age, sex, height, weight, blood pressure, and total cholesterol on each individual. Economic studies may be interested in, among other things, personal income and years of education. As a third example, most university admissions committees ask for an applicant's high school grade point average and standardized admission test scores (e.g., SAT). In this chapter we consider bivariate data, which for now consists of two quantitative variables for each individual. Our first interest is in summarizing such data in a way that is analogous to summarizing univariate (single variable) data.

By way of illustration, let's consider something with which we are all familiar: age. Let’s begin by asking if people tend to marry other people of about the same age. Our experience tells us "yes," but how good is the correspondence? One way to address the question is to look at pairs of ages for a sample of married couples. @tbl-spousalage below shows the ages of 10 married couples. Going across the columns we see that, yes, husbands and wives tend to be of about the same age, with men having a tendency to be slightly older than their wives. This is no big surprise, but at least the data bear out our experiences, which is not always the case.

|             |     |     |     |     |     |     |     |     |     |     |
|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| **Husband** | 36  | 72  | 37  | 36  | 51  | 50  | 47  | 50  | 37  | 41  |
| **Wife**    | 35  | 67  | 33  | 35  | 50  | 46  | 47  | 42  | 36  | 41  |

: Sample of spousal ages of 10 White American Couples. {#tbl-spousalage}

The pairs of ages in @tbl-spousalage are from a dataset consisting of 282 pairs of spousal ages, too many to make sense of from a table. What we need is a way to summarize the 282 pairs of ages. We know that each variable can be summarized by a histogram (see @fig-histspousalage) and by a mean and standard deviation (See @tbl-spousalagemeansd).

![Histograms of spousal ages.](Images/correlations/histspousalage.jpg){#fig-histspousalage}

+--------+----------+--------+----------+--------+-------------+--------+
|        |          |        | **Mean** |        | **Standard\ |        |
|        |          |        |          |        | Deviation** |        |
+--------+----------+--------+----------+--------+-------------+--------+
|        | Husbands |        | 49       |        | 11          |        |
+--------+----------+--------+----------+--------+-------------+--------+
|        | Wives    |        | 47       |        | 11          |        |
+--------+----------+--------+----------+--------+-------------+--------+

: Means and standard deviations of spousal ages. {#tbl-spousalagemeansd}

Each distribution is fairly skewed with a long right tail. From @tbl-spousalage we see that not all husbands are older than their wives and it is important to see that this fact is lost when we separate the variables. That is, even though we provide summary statistics on each variable, the pairing within couple is lost by separating the variables. We cannot say, for example, based on the means alone what percentage of couples has younger husbands than wives. We have to count across pairs to find this out. Only by maintaining the pairing can meaningful answers be found about couples per se. Another example of information not available from the separate descriptions of husbands and wives' ages is the mean age of husbands with wives of a certain age. For instance, what is the average age of husbands with 45-year-old wives? Finally, we do not know the relationship between the husband's age and the wife's age.

We can learn much more by displaying the bivariate data in a graphical form that maintains the pairing. @fig-scatterplotspousalage shows a scatter plot of the paired ages. The x-axis represents the age of the husband and the y-axis the age of the wife.

![Scatter plot showing wife's age as a function of husband's age, r = 0.97.](Images/correlations/scatterplotspousalage.jpg){#fig-scatterplotspousalage}

There are two important characteristics of the data revealed by @fig-scatterplotspousalage. First, it is clear that there is a strong relationship between the husband's age and the wife's age: the older the husband, the older the wife. When one variable (Y) increases with the second variable (X), we say that $X$ and $Y$ have a **positive association**. Conversely, when $Y$ decreases as $X$ increases, we say that they have a **negative association**.

Second, the points cluster along a straight line. When this occurs, the relationship is called a **linear relationship**.

@fig-scatterplotstrength shows a scatter plot of Arm Strength and Grip Strength from 149 individuals working in physically demanding jobs including electricians, construction and maintenance workers, and auto mechanics. Not surprisingly, the stronger someone's grip, the stronger their arm tends to be. There is therefore a positive association between these variables. Although the points cluster along a line, they are not clustered quite as closely as they are for the scatter plot of spousal age.

![Scatter plot of Grip Strength and Arm Strength, r = 0.63.](Images/correlations/scatterplotstrength.jpg){#fig-scatterplotstrength}

Not all scatter plots show linear relationships. @fig-scatterplotgalileo shows the results of an experiment conducted by Galileo on projectile motion.[^correlation-2] In the experiment, Galileo rolled balls down an incline and measured how far they traveled as a function of the release height. It is clear from @fig-scatterplotgalileo that the relationship between "Release Height" and "Distance Traveled" is not described well by a straight line: If you drew a line connecting the lowest point and the highest point, all of the remaining points would be above the line. The data are better fit by a parabola (a type of curved line).

[^correlation-2]: <https://www.amstat.org/publications/jse/v3n1/datasets.dickey.html>

![Galileo's data showing a non-linear relationship.](Images/correlations/scatterplotgalileo.jpg){#fig-scatterplotgalileo}

Scatter plots that show linear relationships between variables can differ in several ways including the slope of the line about which they cluster and how tightly the points cluster about the line. We now turn our attention to a statistical measure of the strength of the relationship between two quantitative variables.

## What is Correlation?[^correlation-3]

[^correlation-3]: This section is adapted from David M. Lane. “Values of the Pearson Correlation.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/describing_bivariate_data/pearson.html>

The Pearson product-moment correlation coefficient is a measure of the strength of the linear relationship between two variables. It is referred to as Pearson's correlation or simply as the **correlation coefficient**. If the relationship between the variables is not linear, then the correlation coefficient does not adequately represent the strength of the relationship between the variables.

The symbol for Pearson's correlation is "$\rho$" when it is measured in the population and "r" when it is measured in a sample. Because we will be dealing almost exclusively with samples, we will use r to represent Pearson's correlation unless otherwise noted.

Pearson's r can range from -1 to 1. An r of -1 indicates a perfect negative linear relationship between variables, an r of 0 indicates no linear relationship between variables, and an r of 1 indicates a perfect positive linear relationship between variables. @fig-poslinearrel shows a scatter plot for which r = 1.

![A perfect positive linear relationship, r = 1.](Images/correlations/poslinearrel.jpg){#fig-poslinearrel}

![A perfect negative linear relationship, r = 1.](Images/correlations/neglinearrel.jpg){#fig-neglinearrel}

![A scatter plot for which r = 0. Notice that there is no relationship between $X$ and $Y$.](Images/correlations/scatterplotnorel.jpg){#fig-scatterplotnorel}

With real data, you would not expect to get values of r of exactly -1, 0, or 1. The data for spousal ages shown earlier in this chapter in @fig-scatterplotspousalage has an r of 0.97.

The relationship between grip strength and arm strength depicted in @fig-scatterplotstrength (also described in the introductory section) is 0.63.

## How Correlation is Calculated[^correlation-4]

[^correlation-4]: This section is adapted from David M. Lane. “Computing Pearson's r.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/describing_bivariate_data/calculation.html>

There are several formulas that can be used to compute Pearson's correlation. Some formulas make more conceptual sense whereas others are easier to actually compute. We are going to begin with a formula that makes more conceptual sense.

We are going to compute the correlation between the variables $X$ and $Y$ shown in @tbl-rcalulation. We begin by computing the mean for $X$ and subtracting this mean from all values of $X$. The new variable is called "x." The variable "y" is computed similarly. The variables x and y are said to be deviation scores because each score is a deviation from the mean. Notice that the means of x and y are both 0. Next we create a new column by multiplying x and y.

Before proceeding with the calculations, let's consider why the sum of the xy column reveals the relationship between $X$ and $Y$. If there were no relationship between $X$ and $Y$, then positive values of x would be just as likely to be paired with negative values of y as with positive values. This would make negative values of xy as likely as positive values and the sum would be small. On the other hand, consider @tbl-rcalulation in which high values of $X$ are associated with high values of $Y$ and low values of $X$ are associated with low values of $Y$. You can see that positive values of x are associated with positive values of y and negative values of x are associated with negative values of y. In all cases, the product of x and y is positive, resulting in a high total for the xy column. Finally, if there were a negative relationship then positive values of x would be associated with negative values of y and negative values of x would be associated with positive values of y. This would lead to negative values for xy.

+-------+-------+-------+-------+-------+--------+----------+----------+
|       | **X** | **Y** | **x** | **y** | **xy** | **x^2^** | **y^2^** |
+-------+-------+-------+-------+-------+--------+----------+----------+
|       |  1    |  4    | -3    | -5    | 15     |  9       | 25       |
+-------+-------+-------+-------+-------+--------+----------+----------+
|       |  3    |  6    | -1    | -3    |  3     |  1       |  9       |
+-------+-------+-------+-------+-------+--------+----------+----------+
|       |  5    | 10    |  1    |  1    |  1     |  1       |  1       |
+-------+-------+-------+-------+-------+--------+----------+----------+
|       |  5    | 12    |  1    |  3    |  3     |  1       |  9       |
+-------+-------+-------+-------+-------+--------+----------+----------+
|       |  6    | 13    |  2    |  4    |  8     |  4       | 16       |
+-------+-------+-------+-------+-------+--------+----------+----------+
| Total | 20    | 45    |  0    |  0    | 30     | 16       | 60       |
+-------+-------+-------+-------+-------+--------+----------+----------+
| Mean  |  4    |  9    |  0    |  0    |  6     |          |          |
+-------+-------+-------+-------+-------+--------+----------+----------+

: Calculation of r. {#tbl-rcalulation}

Pearson's r is designed so that the correlation between height and weight is the same whether height is measured in inches or in feet. To achieve this property, Pearson's correlation is computed by dividing the sum of the xy column ($\sum{xy}$) by the square root of the product of the sum of the $x^2$ column ($\sum{x^2}$) and the sum of the $y^2$ column ($\sum{y^2}$). The resulting formula is:

$$
r = \frac{\sum{xy}}{\sqrt{\sum{x^2}\sum{y^2}}}\
$$

and therefore:

$$
r = \frac{30}{\sqrt{(16)(60)}} = \frac{30}{\sqrt{960}} = \frac{30}{30.984} = 0.968\
$$

An alternative computational formula that avoids the step of computing deviation scores is:

$$
r = \frac{\sum{XY}-\frac{\sum{X}\sum{Y}}{n}}{\sqrt{\left(\sum{X^2}-\frac{(\sum{X})^2}{n}\right)}\sqrt{\left(\sum{Y^2}-\frac{(\sum{Y})^2}{n}\right)}}
$$

## Introduction to Linear Regression[^correlation-5]

[^correlation-5]: This section is adapted from David M. Lane. “Introduction to Linear Regression.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/regression/intro.html>

In simple linear regression, we predict scores on one variable from the scores on a second variable. The variable we are predicting is called the **dependent variable** and is referred to as $Y$. The variable we are basing our predictions on is called the **independent variable** and is referred to as $X$. When there is only one predictor variable, the prediction method is called simple regression. In simple linear regression, the topic of this section, the predictions of $Y$ when plotted as a function of $X$ form a straight line.

The example data in @tbl-exampledata-regression are plotted in @fig-splotexampledata. You can see that there is a positive relationship between $X$ and $Y$. If you were going to predict $Y$ from $X$, the higher the value of $X$, the higher your prediction of $Y$.

+-----+------+-----+------+-----+
|     | $X$  |     | $Y$  |     |
+-----+------+-----+------+-----+
|     | 1.00 |     | 1.00 |     |
+-----+------+-----+------+-----+
|     | 2.00 |     | 2.00 |     |
+-----+------+-----+------+-----+
|     | 3.00 |     | 1.30 |     |
+-----+------+-----+------+-----+
|     | 4.00 |     | 3.75 |     |
+-----+------+-----+------+-----+
|     | 5.00 |     | 2.25 |     |
+-----+------+-----+------+-----+

: Example data. {#tbl-exampledata-regression}

![A scatter plot of the example data.](Images/correlations/splotexampledata.jpg){#fig-splotexampledata}

Linear regression consists of finding the best-fitting straight line through the points. The best-fitting line is called a regression line. The black diagonal line in @fig-splotexampledatadetail is the regression line and consists of the predicted score on $Y$ for each possible value of $X$. The vertical lines from the points to the regression line represent the errors of prediction. As you can see, the red point is very near the regression line; its error of prediction is small. By contrast, the yellow point is much higher than the regression line and therefore its error of prediction is large.

![A scatter plot of the example data. The black line consists of the predictions, the points are the actual data, and the vertical lines between the points and the black line represent errors of prediction.](Images/correlations/splotexampledatadetail.jpg){#fig-splotexampledatadetail}

The error of prediction for a point is the value of the point minus the predicted value (the value on the line). @tbl-exampledatatable shows the predicted values ($\hat{Y}$) and the errors of prediction ($Y-\hat{Y}$). For example, the first point has a $Y$ of 1.00 and a predicted $Y$ (called $\hat{Y}$) of 1.21. Therefore, its error of prediction is -0.21.

+--------+--------+--------+-----------+-------------+-----------------+--------+
|        | $X$    | $Y$    | $\hat{Y}$ | $Y-\hat{Y}$ | $(Y-\hat{Y})^2$ |        |
+--------+--------+--------+-----------+-------------+-----------------+--------+
|        | 1.00   | 1.00   | 1.210     | -0.210      | 0.044           |        |
+--------+--------+--------+-----------+-------------+-----------------+--------+
|        | 2.00   | 2.00   | 1.635     | 0.365       | 0.133           |        |
+--------+--------+--------+-----------+-------------+-----------------+--------+
|        | 3.00   | 1.30   | 2.060     | -0.760      | 0.578           |        |
+--------+--------+--------+-----------+-------------+-----------------+--------+
|        | 4.00   | 3.75   | 2.485     | 1.265       | 1.600           |        |
+--------+--------+--------+-----------+-------------+-----------------+--------+
|        | 5.00   | 2.25   | 2.910     | -0.660      | 0.436           |        |
+--------+--------+--------+-----------+-------------+-----------------+--------+

: Example data. {#tbl-exampledatatable}

You may have noticed that we did not specify what is meant by "best-fitting line." By far, the most commonly-used criterion for the best-fitting line is the line that minimizes the sum of the squared errors of prediction. That is the criterion that was used to find the line in @fig-splotexampledatadetail. The last column in @tbl-exampledatatable shows the squared errors of prediction. The sum of the squared errors of prediction shown in @tbl-exampledatatable is lower than it would be for any other regression line.

The formula for a regression line is

$$
\hat{Y} = \alpha + \beta X
$$

where $\hat{Y}$ is the predicted score, $\alpha$ is the $Y$-intercept, and $\beta$ is the slope of the line. The equation for the line in @fig-splotexampledatadetail is

$$
\hat{Y} = 0.785 + 0.425X
$$

Using this equation, we can calculate predictions for $Y$ based on the value of $X$. For $X$ = 1,

$$
\hat{Y} = 0.785 + (0.425)(1) = 1.21.
$$

For $X$ = 2,

$$
\hat{Y} = 0.785 + (0.425)(2) = 1.64.
$$

### A Real Example

The case study "SAT and College GPA"[^correlation-6] contains high school and university grades for 105 computer science majors at a local state school. We now consider how we could predict a student's university GPA if we knew his or her high school GPA.

[^correlation-6]: <http://onlinestatbook.com/2/case_studies/sat.html>

@fig-unihighschoolgpa shows a scatter plot of University GPA as a function of High School GPA. You can see from the figure that there is a strong positive relationship. The correlation is 0.78. The regression equation is

$$
\widehat{University\_GPA} = (0.675)(High School GPA) + 1.097
$$

Therefore, a student with a high school GPA of 3 would be predicted to have a university GPA of

$$
\widehat{University\_GPA} = (0.675)(3) + 1.097 = 3.12.
$$

![University GPA as a function of High School GPA.](Images/correlations/unihighschoolgpa.jpg){#fig-unihighschoolgpa}

## @sec-correlation Appendix: Multiple Regression[^correlation-7] {.unnumbered}

[^correlation-7]: This section is adapted from Rudy Guerra and David M. Lane. “Introduction to Multiple Regression.” *Online Statistics Education: A Multimedia Course of Study*. <https://onlinestatbook.com/2/regression/multiple_regression.html>

In simple linear regression, a dependent variable is predicted from one independent variable. In multiple regression, the dependent variable is predicted by two or more variables. For example, in the SAT case study, you might want to predict a student's university grade point average on the basis of their High-School GPA (HSGPA) and their total SAT score (verbal + math). The basic idea is to find a linear combination[^correlation-8] of HSGPA and SAT that best predicts University GPA (UGPA). That is, the problem is to find the values of b~1~ and b~2~ in the equation shown below that give the best predictions of UGPA. As in the case of simple linear regression, we define the best predictions as the predictions that minimize the squared errors of prediction.

[^correlation-8]: A linear combination of variables is a way of creating a new variable by combining other variables. A linear combination is one in which each variable is multiplied by a coefficient and the are products summed. For example, if\
    $$Y = 3X_1 + 2X_2 + .5X_3$$\
    then $Y$ is a linear combination of the variables $X_1$, $X_2$, and $X_3$.

$$\widehat{UGPA} = \alpha + \beta_1HSGPA + \beta_2SAT$$

where $\widehat{UGPA}$ is the predicted value of University GPA and $\alpha$ is a constant. For these data, the best prediction equation is shown below:

$$\widehat{UGPA} = 0.540 + 0.541 \times HSGPA + 0.008 \times SAT$$

In other words, to compute the prediction of a student's University GPA, you add up (a) 0.540, (b) their High-School GPA multiplied by 0.541, and (c) their SAT multiplied by 0.008. @tbl-multipleregression shows the data and predictions for the first five students in the dataset.

+--------+---------+--------+--------+--------+------------------+--------+
|        |         |        |        |        |                  |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        | $HSGPA$ |        | $SAT$  |        | $\widehat{UGPA}$ |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        |         |        |        |        |                  |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        | 3.45    |        | 1232   |        | 3.38             |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        |         |        |        |        |                  |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        | 2.78    |        | 1070   |        | 2.89             |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        |         |        |        |        |                  |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        | 2.52    |        | 1086   |        | 2.76             |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        |         |        |        |        |                  |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        | 3.67    |        | 1287   |        | 3.55             |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        |         |        |        |        |                  |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        | 3.24    |        | 1130   |        | 3.19             |        |
+--------+---------+--------+--------+--------+------------------+--------+
|        |         |        |        |        |                  |        |
+--------+---------+--------+--------+--------+------------------+--------+

: Data and Predictions {#tbl-multipleregression}

The values of $\beta$ ($\beta_1$ and $\beta_2$) are called "regression coefficients."

The multiple correlation (R) is equal to the correlation between the predicted scores and the actual scores. In this example, it is the correlation between $\widehat{UGPA}$ and $UGPA$, which turns out to be 0.79. That is, R = 0.79. Note that R will never be negative since if there are negative correlations between the predictor variables and the criterion, the regression weights will be negative so that the correlation between the predicted and actual scores will be positive.

### Interpretation of Regression Coefficients {.unnumbered}

A regression coefficient in multiple regression is the slope of the linear relationship between the criterion variable and the part of a predictor variable that is independent of all other predictor variables. In this example, the regression coefficient for HSGPA can be computed by first predicting HSGPA from SAT and saving the errors of prediction (the differences between $HSGPA$ and $\widehat{HSGPA}$). These errors of prediction are called "residuals" since they are what is left over in HSGPA after the predictions from SAT are subtracted, and represent the part of HSGPA that is independent of SAT. These residuals are referred to as HSGPA.SAT, which means they are the residuals in HSGPA after having been predicted by SAT. The correlation between HSGPA.SAT and SAT is necessarily 0.

The final step in computing the regression coefficient is to find the slope of the relationship between these residuals and UGPA. This slope is the regression coefficient for HSGPA. The following equation is used to predict HSGPA from SAT:

$$\widehat{HSGPA} = -1.314 + 0.0036 \times SAT$$

The residuals are then computed as:

$$HSGPA - \widehat{HSGPA}$$

The linear regression equation for the prediction of UGPA by the residuals is

$$\widehat{UGPA} = 3.173 + 0.541 \times HSGPA.SAT$$

Notice that the slope (0.541) is the same value given previously for $\beta_1$ in the multiple regression equation.

This means that the regression coefficient for HSGPA is the slope of the relationship between the dependent variable and the part of HSGPA that is independent of (uncorrelated with) the other independent variables. It represents the change in the dependent variable associated with a change of one in the independent variable when all other independent variables are held constant. Since the regression coefficient for HSGPA is 0.54, this means that, holding SAT constant, a change of one in HSGPA is associated with a change of 0.54 in $\widehat{UGPA}$. If two students had the same SAT and differed in HSGPA by 2, then you would predict they would differ in UGPA by (2)(0.54) = 1.08. Similarly, if they differed by 0.5, then you would predict they would differ by (0.50)(0.54) = 0.27.

The slope of the relationship between the dependent variable and the part of an independent variable that is unique from (independent of) other independent variables is its partial slope. Thus, the regression coefficient of 0.541 for HSGPA and the regression coefficient of 0.008 for SAT are partial slopes. Each partial slope represents the relationship between the independent variable and the dependent variable holding constant all of the other independent variables.

It is difficult to compare the coefficients for different variables directly because they are measured on different scales. A difference of 1 in HSGPA is a fairly large difference, whereas a difference of 1 on the SAT is negligible. Therefore, it can be advantageous to transform the variables so that they are on the same scale. The most straightforward approach is to standardize the variables (see @sec-standardization) so that they each have a standard deviation of 1. A regression coefficient for standardized variables is called a "standardized coefficient" or "beta coefficient." For these data, the standardized coefficients are 0.625 and 0.198. These values represent the change in the dependent variable (in standard deviations) associated with a change of one standard deviation on an independent variable (holding constant the value(s) on the other independent variable(s)). Clearly, a change of one standard deviation on HSGPA is associated with a larger difference than a change of one standard deviation of SAT. In practical terms, this means that if you know a student's HSGPA, knowing the student's SAT does not aid the prediction of UGPA much. However, if you do not know the student's HSGPA, his or her SAT can aid in the prediction since the standardized coefficient in the simple regression predicting UGPA from SAT is 0.68. For comparison purposes, the standardized coefficient in the simple regression predicting UGPA from HSGPA is 0.78. As is typically the case, the partial slopes are smaller than the slopes in simple regression.
