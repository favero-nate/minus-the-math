{"title":"Sampling Distributions","markdown":{"headingText":"Sampling Distributions","containsRefs":false,"markdown":"\n## Introduction to Sampling Distributions[\\[1\\]](#_ftn1)\n\nSuppose you randomly sampled 10 people from the population of women in Houston, Texas, between the ages of 21 and 35 years and computed the mean height of your sample. You would not expect your sample mean to be equal to the mean of all women in Houston. It might be somewhat lower or it might be somewhat higher, but it would not equal the population mean exactly. Similarly, if you took a second sample of 10 people from the same population, you would not expect the mean of this second sample to equal the mean of the first sample.\n\nRecall that inferential statistics concern generalizing from a sample to a population. A critical part of inferential statistics involves determining how far sample statistics are likely to vary from each other and from the population **parameter**. (In this example, the sample statistics are the sample means and the population parameter is the population mean.) As the later portions of this chapter show, these determinations are based on sampling distributions.\n\n### Discrete Distributions\n\nWe will illustrate the concept of sampling distributions with a simple example. @fig-poolballs shows three pool balls, each with a number on it. Two of the balls are selected randomly (with replacement) and the average of their numbers is computed.\n\n![The pool balls.](Images/Ch6/poolballs.jpg){#fig-poolballs}\n\nAll possible outcomes are shown below in @tbl-twoballsoutcomes.\n\n|     |             |     |            |     |            |     |          |     |\n|-----|-------------|-----|------------|-----|------------|-----|----------|-----|\n|     | **Outcome** |     | **Ball 1** |     | **Ball 2** |     | **Mean** |     |\n|     | 1           |     | 1          |     | 1          |     | 1.0      |     |\n|     | 2           |     | 1          |     | 2          |     | 1.5      |     |\n|     | 3           |     | 1          |     | 3          |     | 2.0      |     |\n|     | 4           |     | 2          |     | 1          |     | 1.5      |     |\n|     | 5           |     | 2          |     | 2          |     | 2.0      |     |\n|     | 6           |     | 2          |     | 3          |     | 2.5      |     |\n|     | 7           |     | 3          |     | 1          |     | 2.0      |     |\n|     | 8           |     | 3          |     | 2          |     | 2.5      |     |\n|     | 9           |     | 3          |     | 3          |     | 3.0      |     |\n\n: All possible outcomes when two balls are sampled with replacement. {#tbl-twoballsoutcomes}\n\nNotice that all the means are either 1.0, 1.5, 2.0, 2.5, or 3.0. The frequencies of these means are shown in Table 6-2. The relative frequencies are equal to the frequencies divided by nine because there are nine possible outcomes.\n\n|     |          |     |               |     |                        |     |\n|-----|----------|-----|---------------|-----|------------------------|-----|\n|     | **Mean** |     | **Frequency** |     | **Relative Frequency** |     |\n|     | 1.0      |     | 1             |     | 0.111                  |     |\n|     | 1.5      |     | 2             |     | 0.222                  |     |\n|     | 2.0      |     | 3             |     | 0.333                  |     |\n|     | 2.5      |     | 2             |     | 0.222                  |     |\n|     | 3.0      |     | 1             |     | 0.111                  |     |\n\n: Frequencies of means for N = 2. {#tbl-meansfreqs}\n\n@fig-meansdist shows a relative frequency distribution of the means based on @tbl-meansfreqs. This distribution is also a probability distribution since the Y-axis is the probability of obtaining a given mean from a sample of two balls in addition to being the relative frequency.\n\n![Distribution of means for N = 2.](Images/Ch6/meansdist.jpg){#fig-meansdist}\n\nThe distribution shown in @fig-meansdist is called the sampling distribution of the mean. Specifically, it is the sampling distribution of the mean for a sample size of 2 (N = 2). For this simple example, the distribution of pool balls and the sampling distribution are both discrete distributions. The pool balls have only the values 1, 2, and 3, and a sample mean can have one of only five values shown in @tbl-meansfreqs.\n\nThere is an alternative way of conceptualizing a sampling distribution that will be useful for more complex distributions. Imagine that two balls are sampled (with replacement) and the mean of the two balls is computed and recorded. Then this process is repeated for a second sample, a third sample, and eventually thousands of samples. After thousands of samples are taken and the mean computed for each, a relative frequency distribution is drawn. The more samples, the closer the relative frequency distribution will come to the sampling distribution shown in @fig-meansdist. As the number of samples approaches infinity, the relative frequency distribution will approach the sampling distribution. This means that you can conceive of a sampling distribution as being a relative frequency distribution based on a very large number of samples. To be strictly correct, the relative frequency distribution approaches the sampling distribution as the number of samples approaches infinity.\n\nIt is important to keep in mind that every statistic, not just the mean, has a sampling distribution. For example, @tbl-alloutcomestwoballs shows all possible outcomes for the range of two numbers (larger number minus the smaller number). @tbl-rangesfreqs shows the frequencies for each of the possible ranges and @fig-rangesdistn2 shows the sampling distribution of the range.\n\n|     |             |     |            |     |            |     |           |     |\n|-----|-------------|-----|------------|-----|------------|-----|-----------|-----|\n|     | **Outcome** |     | **Ball 1** |     | **Ball 2** |     | **Range** |     |\n|     | 1           |     | 1          |     | 1          |     | 0         |     |\n|     | 2           |     | 1          |     | 2          |     | 1         |     |\n|     | 3           |     | 1          |     | 3          |     | 2         |     |\n|     | 4           |     | 2          |     | 1          |     | 1         |     |\n|     | 5           |     | 2          |     | 2          |     | 0         |     |\n|     | 6           |     | 2          |     | 3          |     | 1         |     |\n|     | 7           |     | 3          |     | 1          |     | 2         |     |\n|     | 8           |     | 3          |     | 2          |     | 1         |     |\n|     | 9           |     | 3          |     | 3          |     | 0         |     |\n\n: All possible outcomes when two balls are sampled with replacement. {#tbl-alloutcomestwoballs}\n\n|     |           |     |               |     |                        |     |\n|-----|-----------|-----|---------------|-----|------------------------|-----|\n|     | **Range** |     | **Frequency** |     | **Relative Frequency** |     |\n|     | 0         |     | 3             |     | 0.333                  |     |\n|     | 1         |     | 4             |     | 0.444                  |     |\n|     | 2         |     | 2             |     | 0.222                  |     |\n\n: Distribution of ranges for N = 2. {#tbl-rangesfreqs}\n\n![Distribution of ranges for N = 2.](Images/Ch6/rangesdistn2.jpg){#fig-rangesdistn2}\n\nIt is also important to keep in mind that there is a sampling distribution for various sample sizes. For simplicity, we have been using N = 2. The sampling distribution of the range for N = 3 is shown in @fig-rangesdistn3.\n\n![Distribution of ranges for N = 3.](Images/Ch6/rangesdistn3.jpg){#fig-rangesdistn3}\n\n### Continuous Distributions\n\nIn the previous section, the population consisted of three pool balls. Now we will consider sampling distributions when the population distribution is continuous. What if we had a thousand pool balls with numbers ranging from 0.001 to 1.000 in equal steps? (Although this distribution is not really continuous, it is close enough to be considered continuous for practical purposes.) As before, we are interested in the distribution of means we would get if we sampled two balls and computed the mean of these two balls. In the previous example, we started by computing the mean for each of the nine possible outcomes. This would get a bit tedious for this example since there are 1,000,000 possible outcomes (1,000 for the first ball x 1,000 for the second). Therefore, it is more convenient to use our second conceptualization of sampling distributions which conceives of sampling distributions in terms of relative frequency distributions. Specifically, the relative frequency distribution that would occur if samples of two balls were repeatedly taken and the mean of each sample computed.\n\nWhen we have a truly continuous distribution, it is not only impractical but actually impossible to enumerate all possible outcomes. Moreover, in continuous distributions, the probability of obtaining any single value is zero. Therefore, these values are called probability densities rather than probabilities.\n\n### Sampling Distributions and Inferential Statistics\n\nAs we stated in the beginning of this chapter, **sampling distributions** are important for inferential statistics. In the examples given so far, a population was specified and the sampling distribution of the mean and the range were determined. In practice, the process proceeds the other way: you collect sample data and from these data you estimate parameters of the sampling distribution. This knowledge of the sampling distribution can be very useful. For example, knowing the degree to which means from different samples would differ from each other and from the population mean would give you a sense of how close your particular sample mean is likely to be to the population mean. Fortunately, this information is directly available from a sampling distribution. The most common measure of how much sample means differ from each other is the standard deviation of the sampling distribution of the mean. This standard deviation is called the standard error of the mean. If all the sample means were very close to the population mean, then the standard error of the mean would be small. On the other hand, if the sample means varied considerably, then the standard error of the mean would be large.\n\nTo be specific, assume your sample mean were 125 and you estimated that the standard error of the mean were 5 (using a method shown in a later section). If you had a normal distribution, then it would be likely that your sample mean would be within 10 units of the population mean since most of a normal distribution is within two standard deviations of the mean.\n\nKeep in mind that all statistics have sampling distributions, not just the mean. In later sections we will be discussing the sampling distribution of the variance, the sampling distribution of the difference between means, and the sampling distribution of Pearson's correlation, among others.\n\n## Sampling Distribution of the Mean[\\[2\\]](#_ftn2)\n\nThe sampling distribution of the mean was defined in the section introducing sampling distributions. This section reviews some important properties of the sampling distribution of the mean introduced in the demonstrations in this chapter.\n\n### Mean\n\nThe mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean μ, then the mean of the sampling distribution of the mean is also μ. The symbol μ~M~ is used to refer to the mean of the sampling distribution of the mean. Therefore, the formula for the mean of the sampling distribution of the mean can be written as:\n\n$$\nμ~M~ = μ\n$$\n\n### Variance\n\nThe variance of the sampling distribution of the mean is computed as follows:\n\n$$\nσ^2_M = \\frac{σ^2}{N}\n$$\n\nThat is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean).[\\[3\\]](#_ftn3) Thus, the larger the sample size, the smaller the variance of the sampling distribution of the mean.\n\nThe **standard error** of the mean is the standard deviation of the sampling distribution of the mean. It is therefore the square root of the variance of the sampling distribution of the mean and can be written as:\n\nThe standard error is represented by a σ because it is a standard deviation. The subscript (M) indicates that the standard error in question is the standard error of the mean.\n\n### Central Limit Theorem\n\nThe central limit theorem states that:\n\n> Given a population with a finite mean $μ$ and a finite non-zero variance $σ^2$, the sampling distribution of the mean approaches a normal distribution with a mean of $μ$ and a variance of $σ^2/N$ as $N$, the sample size, increases.\n\nThe expressions for the mean and variance of the sampling distribution of the mean are not new or remarkable. What is remarkable is that regardless of the shape of the parent population, the sampling distribution of the mean approaches a normal distribution as N increases. @fig-uniformsamplingdist shows the results of the simulation for N = 2 and N = 10. The parent population was a uniform distribution. You can see that the distribution for N = 2 is far from a normal distribution. Nonetheless, it does show that the scores are denser in the middle than in the tails. For N = 10 the distribution is quite close to a normal distribution. Notice that the means of the two distributions are the same, but that the spread of the distribution for N = 10 is smaller.\n\n![A simulation of a sampling distribution. The parent population is uniform. The blue line under \"16\" indicates that 16 is the mean. The red line extends from the mean plus and minus one standard deviation.](Images/Ch6/uniformsamplingdist.png){#fig-uniformsamplingdist}\n\n@fig-nonunifirmsamplingdistsim shows how closely the sampling distribution of the mean approximates a normal distribution even when the parent population is very non-normal. If you look closely you can see that the sampling distributions do have a slight positive skew. The larger the sample size, the closer the sampling distribution of the mean would be to a normal distribution.\n\n![A simulation of a sampling distribution. The parent population is very non-normal.](Images/Ch6/nonunifirmsamplingdistsim.png){#fig-nonunifirmsamplingdistsim}\n\n## Confidence Interval on the Mean[\\[4\\]](#_ftn4)\n\nWhen you compute a confidence interval on the mean, you compute the mean of a sample in order to estimate the mean of the population. Clearly, if you already knew the population mean, there would be no need for a confidence interval. However, to explain how confidence intervals are constructed, we are going to work backwards and begin by assuming characteristics of the population. Then we will show how sample data can be used to construct a confidence interval.\n\nAssume that the weights of 10-year-old children are normally distributed with a mean of 90 and a standard deviation of 36. What is the sampling distribution of the mean for a sample size of 9? Recall from the section on the sampling distribution of the mean that the mean of the sampling distribution is μ and the standard error of the mean is\n\n$$\nσ_M = \\frac{σ}{\\sqrt{N}}\n$$\n\nFor the present example, the sampling distribution of the mean has a mean of 90 and a standard deviation of 36/3 = 12. Note that the standard deviation of a sampling distribution is its standard error. @ig-samplingdistn9 shows this distribution. The shaded area represents the middle 95% of the distribution and stretches from 66.48 to 113.52. These limits were computed by adding and subtracting 1.96 standard deviations to/from the mean of 90 as follows:\n\n$$\n90 - (1.96)(12) = 66.48\n$$\n\n$$\n90 + (1.96)(12) = 113.52\n$$\n\nThe value of 1.96 is based on the fact that 95% of the area of a normal distribution is within 1.96 standard deviations of the mean; 12 is the standard error of the mean.\n\n![The sampling distribution of the mean for N=9. The middle 95% of the distribution is shaded.](Images/Ch6/samplingdistn9.png){#fig-samplingdistn9}\n\n@fig-samplingdistn9 shows that 95% of the means are no more than 23.52 units (1.96 standard deviations) from the mean of 90. Now consider the probability that a sample mean computed in a random sample is within 23.52 units of the population mean of 90. Since 95% of the distribution is within 23.52 of 90, the probability that the mean from any given sample will be within 23.52 of 90 is 0.95. This means that if we repeatedly compute the mean (M) from a sample, and create an interval ranging from M - 23.52 to M + 23.52, this interval will contain the population mean 95% of the time. In general, you compute the 95% confidence interval for the mean with the following formula:\n\n$$\n\\text{Lower limit} = M - Z~.95~σ~M~\n$$\n\n$$\n\\text{Upper limit} = M + Z~.95~σ~M~\n$$\n\nwhere Z~.95~ is the number of standard deviations extending from the mean of a normal distribution required to contain 0.95 of the area and σ~M~ is the standard error of the mean.\n\nIf you look closely at this formula for a confidence interval, you will notice that you need to know the standard deviation (σ) in order to estimate the mean. This may sound unrealistic, and it is. However, computing a confidence interval when σ is known is easier than when σ has to be estimated, and serves a pedagogical purpose. Later in this section we will show how to compute a confidence interval for the mean when σ has to be estimated.\n\nSuppose the following five numbers were sampled from a normal distribution with a standard deviation of 2.5: 2, 3, 5, 6, and 9. To compute the 95% confidence interval, start by computing the mean and standard error:\n\n$$\nM = (2 + 3 + 5 + 6 + 9)/5 = 5.\n$$\n\n$$\nσ_M~=\\frac{2.5}{\\sqrt5}=1.118. \n$$\n\nZ~.95~ can be found using the normal distribution calculator[\\[5\\]](#_ftn5) and specifying that the shaded area is 0.95 and indicating that you want the area to be between the cutoff points. As shown in @fig-normaldistcalc, the value is 1.96. If you had wanted to compute the 99% confidence interval, you would have set the shaded area to 0.99 and the result would have been 2.58.\n\n![95% of the area is between -1.96 and 1.96.](Images/Ch6/normaldistcalc.png){#fig-normaldistcalc}\n\nThe confidence interval can then be computed as follows:\n\n$$\n\\text{Lower limit} = 5 - (1.96)(1.118)= 2.81\n$$\n\n$$\n\\text{Upper limit} = 5 + (1.96)(1.118)= 7.19\n$$\n\nYou should use the t distribution rather than the normal distribution when the variance is not known and has to be estimated from sample data. You will learn more about the t distribution in the next section. When the sample size is large, say 100 or above, the t distribution is very similar to the standard normal distribution. However, with smaller sample sizes, the t distribution has relatively more scores in its tails than does the normal distribution. As a result, you have to extend farther from the mean to contain a given proportion of the area. Recall that with a normal distribution, 95% of the distribution is within 1.96 standard deviations of the mean. Using the t distribution, if you have a sample size of only 5, 95% of the area is within 2.78 standard deviations of the mean. Therefore, the standard error of the mean would be multiplied by 2.78 rather than 1.96.\n\nThe values of t to be used in a confidence interval can be looked up in a table of the t distribution. A small version of such a table is shown in @tbl-abbrevttable1. The first column, df, stands for degrees of freedom, and for confidence intervals on the mean, df is equal to N - 1, where N is the sample size.\n\n|     |        |     |          |     |          |     |\n|-----|--------|-----|----------|-----|----------|-----|\n|     | **df** |     | **0.95** |     | **0.99** |     |\n|     | 2      |     | 4.303    |     | 9.925    |     |\n|     | 3      |     | 3.182    |     | 5.841    |     |\n|     | 4      |     | 2.776    |     | 4.604    |     |\n|     | 5      |     | 2.571    |     | 4.032    |     |\n|     | 8      |     | 2.306    |     | 3.355    |     |\n|     | 10     |     | 2.228    |     | 3.169    |     |\n|     | 20     |     | 2.086    |     | 2.845    |     |\n|     | 50     |     | 2.009    |     | 2.678    |     |\n|     | 100    |     | 1.984    |     | 2.626    |     |\n\n: Abbreviated t table. {#tbl-abbrevttable1}\n\nYou can also use an \"inverse t distribution\" calculator[\\[6\\]](#_ftn6) to find the t values to use in confidence intervals.\n\nAssume that the following five numbers are sampled from a normal distribution: 2, 3, 5, 6, and 9 and that the standard deviation is not known. The first steps are to compute the sample mean and variance:\n\n$$\nM = 5\n$$\n\n$$\ns2 = 7.5\n$$\n\nThe next step is to estimate the standard error of the mean. If we knew the population variance, we could use the following formula:\n\n$$\nσ_M = \\frac{σ}{\\sqrt{N}}\n$$\n\nInstead we compute an estimate of the standard error (s~M~):\n\n$$\ns_M = \\frac{s}{\\sqrt{N}}=1.225\n$$\n\nThe next step is to find the value of t. As you can see from @tbl-abbrevttable1, the value for the 95% interval for df = N - 1 = 4 is 2.776. The confidence interval is then computed just as it is when σ~M~. The only differences are that s~M~ and t rather than σ~M~ and Z are used.\n\n$$\n\\text{Lower limit} = 5 - (2.776)(1.225) = 1.60\n$$\n\n$$\n\\text{Upper limit} = 5 + (2.776)(1.225) = 8.40\n$$\n\nMore generally, the formula for the 95% confidence interval on the mean is:\n\n$$\n\\text{Lower limit} = M - (t_{CL})(s_M)\n$$\n\n$$\n\\text{Upper limit} = M + (t_{CL})(s_M)\n$$\n\nwhere $M$ is the sample mean, $t_{CL}$ is the t for the confidence level desired (0.95 in the above example), and $s_M$ is the estimated standard error of the mean.\n\nWe will finish with an analysis of the Stroop Data.[\\[7\\]](#_ftn7) Specifically, we will compute a confidence interval on the mean difference score. Recall that 47 subjects named the color of ink that words were written in. The names conflicted so that, for example, they would name the ink color of the word \"blue\" written in red ink. The correct response is to say \"red\" and ignore the fact that the word is \"blue.\" In a second condition, subjects named the ink color of colored rectangles.\n\n|     |                              |     |                  |     |                |     |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|     | **Naming Colored Rectangle** |     | **Interference** |     | **Difference** |     |\n|     | 17                           |     | 38               |     | 21             |     |\n|     | 15                           |     | 58               |     | 43             |     |\n|     | 18                           |     | 35               |     | 17             |     |\n|     | 20                           |     | 39               |     | 19             |     |\n|     | 18                           |     | 33               |     | 15             |     |\n|     | 20                           |     | 32               |     | 12             |     |\n|     | 20                           |     | 45               |     | 25             |     |\n|     | 19                           |     | 52               |     | 33             |     |\n|     | 17                           |     | 31               |     | 14             |     |\n|     | 21                           |     | 29               |     | 8              |     |\n\n: Response times in seconds for 10 subjects. {#tbl-responsetime}\n\n@tbl-responsetime shows the time difference between the interference and color-naming conditions for 10 of the 47 subjects. The mean time difference for all 47 subjects is 16.362 seconds and the standard deviation is 7.470 seconds. The standard error of the mean is 1.090. A t table shows the critical value of t for 47 - 1 = 46 degrees of freedom is 2.013 (for a 95% confidence interval). Therefore the confidence interval is computed as follows:\n\n$$\n\\text{Lower limit} = 16.362 - (2.013)(1.090) = 14.17\n$$\n\n$$\n\\text{Upper limit} = 16.362 + (2.013)(1.090) = 18.56\n$$\n\nTherefore, the interference effect (difference) for the whole population is likely to be between 14.17 and 18.56 seconds.\n\n## The T Distribution[\\[8\\]](#_ftn8)\n\nIn the introduction to normal distributions it was shown that 95% of the area of a normal distribution is within 1.96 standard deviations of the mean. Therefore, if you randomly sampled a value from a normal distribution with a mean of 100, the probability it would be within 1.96σ of 100 is 0.95. Similarly, if you sample N values from the population, the probability that the sample mean (M) will be within 1.96 σ~M~ of 100 is 0.95.\n\nNow consider the case in which you have a normal distribution but you do not know the standard deviation. You sample N values and compute the sample mean (M) and estimate the standard error of the mean (σ~M~) with s~M~. What is the probability that M will be within 1.96 s~M~ of the population mean (μ)? This is a difficult problem because there are two ways in which M could be more than 1.96 s~M~ from μ: (1) M could, by chance, be either very high or very low and (2) s~M~ could, by chance, be very low. Intuitively, it makes sense that the probability of being within 1.96 standard errors of the mean should be smaller than in the case when the standard deviation is known (and cannot be underestimated). But exactly how much smaller? Fortunately, the way to work out this type of problem was solved in the early 20th century by W. S. Gosset who determined the distribution of a mean divided by an estimate of its standard error. This distribution is called the *Student's t distribution* or sometimes just the t distribution. Gosset worked out the t distribution and associated statistical tests while working for a brewery in Ireland. Because of a contractual agreement with the brewery, he published the article under the pseudonym \"Student.\" That is why the t test is called the \"Student's t test.\"\n\nThe **t distribution** is very similar to the normal distribution when the estimate of variance is based on a large sample, but the t distribution has relatively more scores in its tails when there is a small sample. When working with the t distribution, sample size is expressed in what are called degrees of freedom. Degrees of freedom will be discussed in more detail at the end of this chapter, but if we are estimating the standard error for a sample mean estimate, the degrees of freedom is simply equal to the sample size minus one (N-1).\n\n@fig-tdiststandardsd shows t distributions with 2, 4, and 10 degrees of freedom and the standard normal distribution. Notice that the normal distribution has relatively more scores in the center of the distribution and the t distribution has relatively more in the tails. The t distribution approaches the normal distribution as the degrees of freedom increase.\n\n![A comparison of t distributions with 2, 4, and 10 df and the standard normal distribution. The distribution with the lowest peak is the 2 df distribution, the next lowest is 4 df, the lowest after that is 10 df, and the highest is the standard normal distribution.](Images/Ch6/tdiststandardsd.jpg){#fig-tdiststandardsd}\n\nSince the t distribution has more area in the tails, the percentage of the distribution within 1.96 standard deviations of the mean is less than the 95% for the normal distribution. @tbl-abbrevttable2 shows the number of standard deviations from the mean required to contain 95% and 99% of the area of the t distribution for various degrees of freedom. These are the values of t that you use in a confidence interval. The corresponding values for the normal distribution are 1.96 and 2.58 respectively. Notice that with few degrees of freedom, the values of t are much higher than the corresponding values for a normal distribution and that the difference decreases as the degrees of freedom increase. The values shown in Table 6-7 can be obtained from statistical software or an online calculator.[\\[9\\]](#_ftn9)\n\n|     |        |     |          |     |          |     |\n|-----|--------|-----|----------|-----|----------|-----|\n|     | **df** |     | **0.95** |     | **0.99** |     |\n|     | 2      |     | 4.303    |     | 9.925    |     |\n|     | 3      |     | 3.182    |     | 5.841    |     |\n|     | 4      |     | 2.776    |     | 4.604    |     |\n|     | 5      |     | 2.571    |     | 4.032    |     |\n|     | 8      |     | 2.306    |     | 3.355    |     |\n|     | 10     |     | 2.228    |     | 3.169    |     |\n|     | 20     |     | 2.086    |     | 2.845    |     |\n|     | 50     |     | 2.009    |     | 2.678    |     |\n|     | 100    |     | 1.984    |     | 2.626    |     |\n\n: Abbreviated t table. {#tbl-abbrevttable2}\n\nReturning to the problem posed at the beginning of this section, suppose you sampled 9 values from a normal population and estimated the standard error of the mean (σ~M~) with s~M~. What is the probability that M would be within 1.96s~M~ of μ? Since the sample size is 9, there are N - 1 = 8 df. From @tbl-abbrevttable2, you can see that with 8 df the probability is 0.95 that the mean will be within 2.306 s~M~ of μ. The probability that it will be within 1.96 s~M~ of μ is therefore lower than 0.95.\n\nAs shown in @fig-tdist8df, a t distribution calculator[\\[10\\]](#_ftn10) can be used to find that 0.086 of the area of a t distribution is more than 1.96 standard deviations from the mean, so the probability that M would be less than 1.96sM from μ is 1 - 0.086 = 0.914.\n\n![Area more than 1.96 standard deviations from the mean in a t distribution with 8 df. Note that the two-tailed button is selected so that the area in both tails will be included.](Images/Ch6/tdist8df.png){#fig-tdist8df}\n\nAs expected, this probability is less than 0.95 that would have been obtained if σM had been known instead of estimated.\n\n## Degrees of Freedom[\\[11\\]](#_ftn11)\n\nSome estimates are based on more information than others. For example, an estimate of the variance based on a sample size of 100 is based on more information than an estimate of the variance based on a sample size of 5. The **degrees of freedom (df)** of an estimate is the number of independent pieces of information on which the estimate is based.\n\nAs an example, let's say that we know that the mean height of Martians is 6 and wish to estimate the variance of their heights. We randomly sample one Martian and find that its height is 8. Recall that the variance is defined as the mean squared deviation of the values from their population mean. We can compute the squared deviation of our value of 8 from the population mean of 6 to find a single squared deviation from the mean. This single squared deviation from the mean, (8-6)^2^ = 4, is an estimate of the mean squared deviation for all Martians. Therefore, based on this sample of one, we would estimate that the population variance is 4. This estimate is based on a single piece of information and therefore has 1 df. If we sampled another Martian and obtained a height of 5, then we could compute a second estimate of the variance, (5-6)^2^ = 1. We could then average our two estimates (4 and 1) to obtain an estimate of 2.5. Since this estimate is based on two independent pieces of information, it has two degrees of freedom. The two estimates are independent because they are based on two independently and randomly selected Martians. The estimates would not be independent if after sampling one Martian, we decided to choose its brother as our second Martian.\n\nAs you are probably thinking, it is pretty rare that we know the population mean when we are estimating the variance. Instead, we have to first estimate the population mean (μ) with the sample mean (M). The process of estimating the mean affects our degrees of freedom as shown below.\n\nReturning to our problem of estimating the variance in Martian heights, let's assume we do not know the population mean and therefore we have to estimate it from the sample. We have sampled two Martians and found that their heights are 8 and 5. Therefore M, our estimate of the population mean, is\n\n$$\nM = (8+5)/2 = 6.5.\n$$\n\nWe can now compute two estimates of variance:\n\n$$\n\\text{Estimate 1} = (8-6.5)^2 = 2.25\n$$\n\n$$\n\\text{Estimate 2} = (5-6.5)^2 = 2.25\n$$\n\nNow for the key question: Are these two estimates independent? The answer is no because each height contributed to the calculation of M. Since the first Martian's height of 8 influenced M, it also influenced Estimate 2. If the first height had been, for example, 10, then M would have been 7.5 and Estimate 2 would have been (5-7.5)^2^ = 6.25 instead of 2.25. The important point is that the two estimates are not independent and therefore we do not have two degrees of freedom. Another way to think about the non-independence is to consider that if you knew the mean and one of the scores, you would know the other score. For example, if one score is 5 and the mean is 6.5, you can compute that the total of the two scores is 13 and therefore that the other score must be 13-5 = 8.\n\nIn general, the degrees of freedom for an estimate is equal to the number of values minus the number of parameters estimated en route to the estimate in question. In the Martians example, there are two values (8 and 5) and we had to estimate one parameter (μ) on the way to estimating the parameter of interest (σ^2^). Therefore, the estimate of variance has 2 - 1 = 1 degree of freedom. If we had sampled 12 Martians, then our estimate of variance would have had 11 degrees of freedom. Therefore, the degrees of freedom of an estimate of variance is equal to N - 1, where N is the number of observations.\n\nRecall from the section on variability that the formula for estimating the variance in a sample is:\n\n$$\ns^2=\\frac{Σ(X-M)^2}{N-1}\n$$\n\nThe denominator of this formula is the degrees of freedom.\n\nSo far, we’ve only seen examples where the degrees of freedom is equal to N - 1. But in later chapters, we’ll see examples of statistical inference tools that require estimating more than one parameter en route to the estimate in question, and therefore we’ll need to subtract more than one from the number of observations to get the degrees of freedom. For example, the degrees of freedom might be calculated as N - 5 or N - 3, depending on what we are estimating.\n\n------------------------------------------------------------------------\n\n[\\[1\\]](#_ftnref1) This subsection is adapted from David M. Lane. “Introduction to Sampling Distributions.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/sampling_distributions/intro_samp_dist.html>\n\n[\\[2\\]](#_ftnref2) This subsection is adapted from David M. Lane. “Sampling Distribution of the Mean.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/sampling_distributions/samp_dist_mean.html>\n\n[\\[3\\]](#_ftnref3) This expression can be derived very easily from the variance sum law. Let's begin by computing the variance of the sampling distribution of the sum of three numbers sampled from a population with variance σ^2^. The variance of the sum would be σ^2^ + σ^2^ + σ^2^. For N numbers, the variance would be Nσ^2^. Since the mean is 1/N times the sum, the variance of the sampling distribution of the mean would be 1/N^2^ times the variance of the sum, which equals σ^2^/N.\n\n[\\[4\\]](#_ftnref4) This subsection is adapted from David M. Lane. “Confidence Interval on the Mean.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/estimation/mean.html>\n\n[\\[5\\]](#_ftnref5) <http://onlinestatbook.com/2/calculators/normal_dist.html>\n\n[\\[6\\]](#_ftnref6) <http://onlinestatbook.com/2/calculators/inverse_t_dist.html>\n\n[\\[7\\]](#_ftnref7) <http://onlinestatbook.com/2/case_studies/stroop.html>\n\n[\\[8\\]](#_ftnref8) This section is adapted from David M. Lane. “t Distribution.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/estimation/t_distribution.html>\n\n[\\[9\\]](#_ftnref9) <http://onlinestatbook.com/2/calculators/inverse_t_dist.html>\n\n[\\[10\\]](#_ftnref10) <http://onlinestatbook.com/2/calculators/t_dist.html>\n\n[\\[11\\]](#_ftnref11) This section is adapted from David M. Lane. “Degrees of Freedom.” *Online Statistics Education: A Multimedia Course of Study*. <http://onlinestatbook.com/2/estimation/df.html>\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"sampling-distributions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"sampling-distributions.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"documentclass":"scrreprt","header-includes":["\\publishers{Get the latest version of this text at: \\url{https://minusthemath.com} \\bigskip \\bigskip \\bigskip \\bigskip \\\\\n\\href{https://creativecommons.org/licenses/by/4.0/?ref=chooser-v1}{\\includegraphics{Images/by.png}}}\n"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}