[
  {
    "objectID": "Chapter1.html#variables1",
    "href": "Chapter1.html#variables1",
    "title": "1  Graphical Tools for Describing One Variable at a Time",
    "section": "1.1 Variables[1]",
    "text": "1.1 Variables[1]\nVariables are properties or characteristics of some event, object, or person that can take on different values or amounts (as opposed to constants such as π that do not vary). When conducting research, experimenters often manipulate or measure variables. For example, an experimenter might compare the effectiveness of four types of antidepressants. In this case, the variable is “type of antidepressant.” This experimenter might also ask study participants to indicate their mood on a scale of 1 to 10. “Mood” would be a second variable.\n\n1.1.1 Qualitative and Quantitative Variables\nAn important distinction between variables is between qualitative variables and quantitative variables. Qualitative variables are those that express a qualitative attribute such as hair color, eye color, religion, favorite movie, gender, and so on. The values of a qualitative variable do not imply a numerical ordering. Values of the variable “religion” differ qualitatively; no ordering of religions is implied. Qualitative variables are also sometimes referred to as categorical or nominal variables. Quantitative variables are those variables that are measured in terms of numbers. Some examples of quantitative variables are height, weight, and shoe size.\n\nExample: Can blueberries slow down aging? A study indicates that antioxidants found in blueberries may slow down the process of aging. In this study, 19-month-old rats (equivalent to 60-year-old humans) were fed either their standard diet or a diet supplemented by either blueberry, strawberry, or spinach powder. After eight weeks, the rats were given memory and motor skills tests. Although all supplemented rats showed improvement, those supplemented with blueberry powder showed the most notable improvement.\nMore information: https://www.apa.org/monitor/dec01/blueberries.html\n\nIn the study on the effect of diet discussed above, the independent variable was type of supplement: none, strawberry, blueberry, and spinach. The variable “type of supplement” is a qualitative variable; there is nothing quantitative about it. In contrast, the dependent variable “memory test” is a quantitative variable since memory performance was measured on a quantitative scale (number correct).\n\n\n1.1.2 Discrete and Continuous Variables\nVariables such as number of children in a household are called discrete variables since the possible scores are discrete points on the scale. For example, a household could have three children or six children, but not 4.53 children. Other variables such as “time to respond to a question” are continuous variables since the scale is continuous and not made up of discrete steps. The response time could be 1.64 seconds, or it could be 1.64237123922121 seconds. Of course, the practicalities of measurement preclude most measured variables from being truly continuous."
  },
  {
    "objectID": "Chapter1.html#percentiles2",
    "href": "Chapter1.html#percentiles2",
    "title": "1  Graphical Tools for Describing One Variable at a Time",
    "section": "1.2 Percentiles[2]",
    "text": "1.2 Percentiles[2]\nWe will turn our attention to some basic graphical tools we can use to visualize qualitative and quantitative variables in a moment, but first it is helpful to briefly go over percentiles since they will be used in some graphical tools. Many of us have probably encountered percentiles before in the context of standardized exam testing. A test score in and of itself is usually difficult to interpret. For example, if you learned that your score on a measure of shyness was 35 out of a possible 50, you would have little idea how shy you are compared to other people. More relevant is the percentage of people with lower shyness scores than yours. This percentage is called a percentile. If 65% of the scores were below yours, then your score would be the 65th percentile.\n\n1.2.1 Three Alternative Definitions of Percentile\nThere is no universally accepted definition of a percentile. Using the 65th percentile as an example, the 65th percentile can be defined as the lowest score that is greater than 65% of the scores. This is the way we defined it above and we will call this “Definition 1.” The 65th percentile can also be defined as the smallest score that is greater than or equal to 65% of the scores. This we will call “Definition 2.” Though these two definitions appear very similar, they can sometimes lead to dramatically different results, especially when there is relatively little data. Moreover, neither of these definitions is explicit about how to handle rounding. For instance, what rank is required to be higher than 65% of the scores when the total number of scores is 50? This is tricky because 65% of 50 is 32.5. How do we find the lowest number that is higher than 32.5 of the scores?\nA third way to compute percentiles is a weighted average of the percentiles computed according to the first two definitions. The details of computing percentiles under this third definition are a bit complicated, but fortunately, statistical software can easily do the calculations for us. Since it is unlikely you will need to compute percentiles by hand, we leave the details of these computations to the appendix appearing at the end of this chapter. Despite its complexity, the third definition handles rounding more gracefully than the other two and has the advantage that it allows the median to be defined conveniently as the 50th percentile. Unless otherwise specified, when we refer to “percentile,” we will be referring to this third definition of percentiles."
  },
  {
    "objectID": "Chapter1.html#graphing-qualitative-variables3",
    "href": "Chapter1.html#graphing-qualitative-variables3",
    "title": "1  Graphical Tools for Describing One Variable at a Time",
    "section": "1.3 Graphing Qualitative Variables[3]",
    "text": "1.3 Graphing Qualitative Variables[3]\nWhen Apple Computer introduced the iMac computer in August 1998, the company wanted to learn whether the iMac was expanding Apple’s market share. Was the iMac just attracting previous Macintosh owners? Or was it purchased by newcomers to the computer market and by previous Windows users who were switching over? To find out, 500 iMac customers were interviewed. Each customer was categorized as a previous Macintosh owner, a previous Windows owner, or a new computer purchaser.\nThis section examines graphical methods for displaying the results of the interviews. We’ll learn some general lessons about how to graph data that fall into a small number of categories. A later section will consider how to graph numerical data in which each observation is represented by a number in some range. The key point about the qualitative data that occupy us in the present section is that they do not come with a pre-established ordering (the way numbers are ordered). For example, there is no natural sense in which the category of previous Windows users comes before or after the category of previous Macintosh users. This situation may be contrasted with quantitative data, such as a person’s weight. People of one weight are naturally ordered with respect to people of a different weight.\n\n1.3.1 Frequency Tables\nAll of the graphical methods shown in this section are derived from frequency tables. Table 1.1 shows a frequency table for the results of the iMac study; it shows the frequencies of the various response categories. It also shows the relative frequencies, which are the proportion of responses in each category. For example, the relative frequency for “none” is 85/500 = 0.17.\n\n\nTable 1.1: Frequency Table for the iMac Data.\n\n\nPrevious Ownership\nFrequency\nRelative Frequency\n\n\n\n\nNone\n85\n0.17\n\n\nWindows\n60\n0.12\n\n\nMacintosh\n355\n0.71\n\n\nTotal\n500\n1.00\n\n\n\n\n\n\n1.3.2 Pie Charts\nThe pie chart in Figure 1.1 shows the results of the iMac study. In a pie chart, each category is represented by a slice of the pie. The area of the slice is proportional to the percentage of responses in the category. This is simply the relative frequency multiplied by 100. Although most iMac purchasers were Macintosh owners, Apple was encouraged by the 12% of purchasers who were former Windows users, and by the 17% of purchasers who were buying a computer for the first time.\n\n\n\nFigure 1.1: Pie chart of iMac purchases illustrating frequencies of previous computer ownership.\n\n\nPie charts are effective for displaying the relative frequencies of a small number of categories. They are not recommended, however, when you have a large number of categories. Pie charts can also be confusing when they are used to compare the outcomes of two different surveys or experiments. In an influential book on the use of graphs, Edward Tufte asserted, “The only worse design than a pie chart is several of them.”\nHere is another important point about pie charts. If they are based on a small number of observations, it can be misleading to label the pie slices with percentages. For example, if just 5 people had been interviewed by Apple Computers, and 3 were former Windows users, it would be misleading to display a pie chart with the Windows slice showing 60%. With so few people interviewed, such a large percentage of Windows users might easily have occurred since chance can cause large errors with small samples. In this case, it is better to alert the user of the pie chart to the actual numbers involved. The slices should therefore be labeled with the actual frequencies observed (e.g., 3) instead of with percentages.\n\n\n1.3.3 Bar charts\nBar charts can also be used to represent frequencies of different categories. A bar chart of the iMac purchases is shown in Figure 1.2. Frequencies are shown on the Y-axis and the type of computer previously owned is shown on the X-axis. Typically, the Y-axis shows the number of observations in each category rather than the percentage of observations as is typical in pie charts.\n\n\n\nFigure 1.2: Bar chart of iMac purchases as a function of previous computer ownership.\n\n\n\n1.3.3.1 Comparing Distributions\nOften we need to compare the results of different surveys, or of different conditions within the same overall survey. In this case, we are comparing the “distributions” of responses between the surveys or conditions. Bar charts are often excellent for illustrating differences between two distributions. Figure 1.3 shows the number of people playing card games at the Yahoo website on a Sunday and on a Wednesday in the Spring of 2001. We see that there were more players overall on Wednesday compared to Sunday. The number of people playing Pinochle was nonetheless the same on these two days. In contrast, there were about twice as many people playing hearts on Wednesday as on Sunday. Facts like these emerge clearly from a well-designed bar chart.\n\n\n\nFigure 1.3: A bar chart of the number of people playing different card games on Sunday and Wednesday.\n\n\nThe bars in Figure 1.3 are oriented horizontally rather than vertically. The horizontal format is useful when you have many categories because there is more room for the category labels. We’ll have more to say about bar charts when we consider numerical quantities later in the section Bar Charts.\n\n\n\n1.3.4 Some graphical mistakes to avoid\nDon’t get fancy! People sometimes add features to graphs that don’t help to convey their information. For example, 3-dimensional bar charts such as the one shown in Figure 1.4 are usually not as effective as their two-dimensional counterparts.\n\n\n\nFigure 1.4: A three-dimensional version of Figure 1.2.\n\n\nHere is another way that fanciness can lead to trouble. Instead of plain bars, it is tempting to substitute meaningful images. For example, Figure 1.5 presents the iMac data using pictures of computers. The heights of the pictures accurately represent the number of buyers, yet Figure Figure 1.5 is misleading because the viewer’s attention will be captured by areas. The areas can exaggerate the size differences between the groups. In terms of percentages, the ratio of previous Macintosh owners to previous Windows owners is about 6 to 1. But the ratio of the two areas in Figure 1.5 is about 35 to 1. A biased person wishing to hide the fact that many Windows owners purchased iMacs would be tempted to use Figure 1.5 instead of Figure 1.2! Edward Tufte coined the term “lie factor” to refer to the ratio of the size of the effect shown in a graph to the size of the effect shown in the data. He suggests that lie factors greater than 1.05 or less than 0.95 produce unacceptable distortion.\n\n\n\nFigure 1.5: A redrawing of Figure 1-2 with a lie factor greater than 8.\n\n\nAnother distortion in bar charts results from setting the baseline to a value other than zero. The baseline is the bottom of the Y-axis, representing the least number of cases that could have occurred in a category. Normally, but not always, this number should be zero. Figure 1.6 shows the iMac data with a baseline of 50. Once again, the differences in areas suggest a different story than the true differences in percentages. The percentage of Windows-switchers seems minuscule compared to its true value of 12%.\n\n\n\nFigure 1.6: A redrawing of Figure 1-2 with a baseline of 50.\n\n\nFinally, we note that it is a serious mistake to use a line graph when the X-axis contains merely qualitative variables. A line graph is essentially a bar graph with the tops of the bars represented by points joined by lines (the rest of the bar is suppressed). Figure 1.7 inappropriately shows a line graph of the card game data from Yahoo. The drawback to Figure 1.7 is that it gives the false impression that the games are naturally ordered in a numerical way when, in fact, they are ordered alphabetically.\n\n\n\nFigure 1.7: A line graph used inappropriately to depict the number of people playing different card games on Sunday and Wednesday."
  },
  {
    "objectID": "Chapter1.html#graphing-quantitative-variables",
    "href": "Chapter1.html#graphing-quantitative-variables",
    "title": "1  Graphical Tools for Describing One Variable at a Time",
    "section": "1.4 Graphing Quantitative Variables",
    "text": "1.4 Graphing Quantitative Variables\nHaving considered qualitative variables, we now turn our attention to some of the common types of graphs that are used to depict quantitative variables, beginning with histograms.\n\n1.4.1 Histograms[4]\nA histogram is a graphical method for displaying the shape of a distribution. It is particularly useful when there are a large number of observations. We begin with an example consisting of the scores of 642 students on a psychology test. The test consists of 197 items, each graded as “correct” or “incorrect.” The students’ scores ranged from 46 to 167.\nThe first step is to create a frequency table. Unfortunately, a simple frequency table would be too big, containing over 100 rows. To simplify the table, we group scores together as shown in Table 1.2.\n\n\nTable 1.2: Grouped Frequency Distribution of Psychology Test Scores\n\n\n\n\n\n\n\nInterval’s Lower Limit\nInterval’s Upper Limit\nClass Frequency\n\n\n39.5\n49.5\n3\n\n\n49.5\n59.5\n10\n\n\n59.5\n69.5\n53\n\n\n69.5\n79.5\n107\n\n\n79.5\n89.5\n147\n\n\n89.5\n99.5\n130\n\n\n99.5\n109.5\n78\n\n\n109.5\n119.5\n59\n\n\n119.5\n129.5\n36\n\n\n129.5\n139.5\n11\n\n\n139.5\n149.5\n6\n\n\n149.5\n159.5\n1\n\n\n159.5\n169.5\n1\n\n\n\n\nTo create this table, the range of scores was broken into intervals, called class intervals. The first interval is from 39.5 to 49.5, the second from 49.5 to 59.5, etc. Next, the number of scores falling into each interval was counted to obtain the class frequencies. There are three scores in the first interval, 10 in the second, etc.\nClass intervals of width 10 provide enough detail about the distribution to be revealing without making the graph too “choppy.” More information on choosing the widths of class intervals is presented later in this section. Placing the limits of the class intervals midway between two numbers (e.g., 49.5) ensures that every score will fall in an interval rather than on the boundary between intervals.\nIn a histogram, the class frequencies are represented by bars. The height of each bar corresponds to its class frequency. A histogram of these data is shown in Figure 1.8.\n\n\n\nFigure 1.8: Histogram of scores on a psychology test.\n\n\nThe histogram makes it plain that most of the scores are in the middle of the distribution, with fewer scores in the extremes. You can also see that the distribution is not symmetric: the scores extend to the right farther than they do to the left. The distribution is therefore said to be skewed.\nIn our example, the observations are whole numbers. Histograms can also be used when the scores are measured on a more continuous scale such as the length of time (in milliseconds) required to perform a task. In this case, there is no need to worry about fence-sitters since they are improbable. (It would be quite a coincidence for a task to require exactly 7 seconds, measured to the nearest thousandth of a second.) We are therefore free to choose whole numbers as boundaries for our class intervals, for example, 4000, 5000, etc. The class frequency is then the number of observations that are greater than or equal to the lower bound, and strictly less than the upper bound. For example, one interval might hold times from 4000 to 4999 milliseconds. Using whole numbers as boundaries avoids a cluttered appearance, and is the practice of many computer programs that create histograms. Note also that some computer programs label the middle of each interval rather than the end points.\nHistograms can be based on relative frequencies instead of actual frequencies. Histograms based on relative frequencies show the proportion of scores in each interval rather than the number of scores. In this case, the Y-axis runs from 0 to 1 (or somewhere in between if there are no extreme proportions). You can change a histogram based on frequencies to one based on relative frequencies by (a) dividing each class frequency by the total number of observations, and then (b) plotting the quotients on the Y-axis (labeled as proportion).\nThere is more to be said about the widths of the class intervals, sometimes called bin widths. Your choice of bin width determines the number of class intervals. This decision, along with the choice of starting point for the first interval, affects the shape of the histogram. There are some “rules of thumb” that can help you choose an appropriate width. (But keep in mind that none of the rules is perfect.) We prefer the Rice rule, which is to set the number of intervals to twice the cube root of the number of observations. In the case of 1000 observations, the Rice rule yields 20 intervals. For the psychology test example used above, the Rice rule recommends 17. The best advice is to experiment with different choices of width, and to choose a histogram according to how well it communicates the shape of the distribution.\n\nBox plot terms and values for women’s times.\n\n\n\n\n\n\n\nName\nFormula\nValue\n\n\n\n\nUpper Hinge\n75th Percentile\n20\n\n\nLower Hinge\n25th Percentile\n17\n\n\nH-Spread\nUpper Hinge - Lower Hinge\n3\n\n\nStep\n1.5 x H-Spread\n4.5\n\n\nUpper Inner Fence\nUpper Hinge + 1 Step\n24.5\n\n\nLower Inner Fence\nLower Hinge - 1 Step\n12.5\n\n\nUpper Outer Fence\nUpper Hinge + 2 Steps\n29\n\n\nLower Outer Fence\nLower Hinge - 2 Steps\n8\n\n\nUpper Adjacent\nLargest value below Upper Inner Fence\n24\n\n\nLower Adjacent\nSmallest value above Lower Inner Fence\n14\n\n\nOutside Value\nA value beyond an Inner Fence but not beyond an Outer Fence\n29\n\n\nFar Out Value\nA value beyond an Outer Fence\nNone\n\n\n\n\n\n1.4.2 Box Plots[5]\nBox plots are useful for identifying outliers and for comparing distributions. We will explain box plots with the help of data from an in-class experiment. As part of the “Stroop Interference Case Study,”[6] students in introductory statistics were presented with a page containing 30 colored rectangles. Their task was to name the colors as quickly as possible. Their times (in seconds) were recorded. We’ll compare the scores for the 16 men and 31 women who participated in the experiment by making separate box plots for each gender. Such a display is said to involve parallel box plots.\nThere are several steps in constructing a box plot. The first relies on the 25th, 50th, and 75th percentiles in the distribution of scores. Figure 1-9 shows how these three statistics are used. For each gender, we draw a box extending from the 25th percentile to the 75th percentile. The 50th percentile is drawn inside the box. Therefore, the bottom of each box is the 25th percentile, the top is the 75th percentile, and the line in the middle is the 50th percentile. The data for the women in our sample are shown in Table 1.3.\n\n\nTable 1.3: Women’s times.\n\n\n\n\n\n\n\n\n\n\n\n14\n15\n16\n16\n17\n17\n17\n17\n17\n18\n18\n18\n18\n18\n18\n19\n19\n19\n20\n20\n20\n20\n20\n20\n21\n21\n22\n23\n24\n24\n29\n\n\n\n\nFor these data, the 25th percentile is 17, the 50th percentile is 19, and the 75th percentile is 20. For the men (whose data are not shown), the 25th percentile is 19, the 50th percentile is 22.5, and the 75th percentile is 25.5.\n\n\n\nFigure 1.9: The first step in creating box plots.\n\n\nBefore proceeding, the terminology in Table 1.4 is helpful.\n\n\nTable 1.4: Terminology.\n\n\n\n\n\n\n\nName\nFormula\nValue\n\n\n\n\nUpper Hinge\n75th Percentile\n20\n\n\nLower Hinge\n25th Percentile\n17\n\n\nH-Spread\nUpper Hinge - Lower Hinge\n3\n\n\nStep\n1.5 x H-Spread\n4.5\n\n\nUpper Inner Fence\nUpper Hinge + 1 Step\n24.5\n\n\nLower Inner Fence\nLower Hinge - 1 Step\n12.5\n\n\nUpper Outer Fence\nUpper Hinge + 2 Steps\n29\n\n\nLower Outer Fence\nLower Hinge - 2 Steps\n8\n\n\nUpper Adjacent\nLargest value below Upper Inner Fence\n24\n\n\nLower Adjacent\nSmallest value above Lower Inner Fence\n14\n\n\nOutside Value\nA value beyond an Inner Fence but not beyond an Outer Fence\n29\n\n\nFar Out Value\nA value beyond an Outer Fence\nNone\n\n\n\n\nContinuing with the box plots, we put “whiskers” above and below each box to give additional information about the spread of the data. Whiskers are vertical lines that end in a horizontal stroke. Whiskers are drawn from the upper and lower hinges to the upper and lower adjacent values (24 and 14 for the women’s data).\n\n\n\nFigure 1.10: The box plots with the whiskers drawn.\n\n\nAlthough we don’t draw whiskers all the way to outside or far out values, we still wish to represent them in our box plots. This is achieved by adding additional marks beyond the whiskers. Specifically, outside values are indicated by small “o’s” and far out values are indicated by asterisks (*). In our data, there are no far out values and just one outside value. This outside value of 29 is for the women and is shown in Figure 1.11.\n\n\n\nFigure 1.11: The box plots with the outside value shown.\n\n\nThere is one more mark to include in box plots (although sometimes it is omitted). We indicate the mean score for a group by inserting a plus sign. Figure 1.12 shows the result of adding means to our box plots.\n\n\n\nFigure 1.12: The completed box plots.\n\n\nFigure 1.12 provides a revealing summary of the data. Since half the scores in a distribution are between the hinges (recall that the hinges are the 25th and 75th percentiles), we see that half the women’s times are between 17 and 20 seconds, whereas half the men’s times are between 19 and 25.5. We also see that women generally named the colors faster than the men did, although one woman was slower than almost all of the men. Figure 1.13 shows the box plot for the women’s data with detailed labels.\n\n\n\nFigure 1.13: The box plot for the women’s data with detailed labels.\n\n\nBox plots provide basic information about a distribution. For example, a distribution with a positive skew would have a longer whisker in the positive direction than in the negative direction. A larger mean than median would also indicate a positive skew. Box plots are good at portraying extreme values and are especially good at showing differences between distributions. However, many of the details of a distribution are not revealed in a box plot, and to examine these details one should create a histogram.\n\n\n1.4.3 Variations on box plots\nStatistical analysis programs may offer options on how box plots are created. For example, the box plots in Figure 1.14 are constructed from our data but differ from the previous box plots in several ways.\n\nIt does not mark outliers.\nThe means are indicated by green lines rather than plus signs.\nThe mean of all scores is indicated by a gray line.\nIndividual scores are represented by dots. Since the scores have been rounded to the nearest second, any given dot might represent more than one score.\nThe box for the women is wider than the box for the men because the widths of the boxes are proportional to the number of subjects of each gender (31 women and 16 men).\n\n\n\n\nFigure 1.14: Box plots showing the individual scores and the means.\n\n\nEach dot in Figure 1.14 represents a group of subjects with the same score (rounded to the nearest second). An alternative graphing technique is to “jitter” the points. This means spreading out different dots at the same horizontal position, one dot for each subject. The exact horizontal position of a dot is determined randomly (under the constraint that different dots don’t overlap exactly). Spreading out the dots helps you to see multiple occurrences of a given score. However, depending on the dot size and the screen resolution, some points may be obscured even if the points are jittererd. Figure 1.15 shows what jittering looks like.\n\n\n\nFigure 1.15: Box plots with the individual scores jittered.\n\n\nDifferent styles of box plots are best for different situations, and there are no firm rules for which to use. When exploring your data, you should try several ways of visualizing them. Which graphs you include in your report should depend on how well different graphs reveal the aspects of the data you consider most important.\n\n\n1.4.4 Bar Charts[7]\nIn the section on qualitative variables, we saw how bar charts could be used to illustrate the frequencies of different categories. For example, one bar chart showed how many purchasers of iMac computers were previous Macintosh users, previous Windows users, and new computer purchasers.\nIn this section, we show how bar charts can be used to present other kinds of quantitative information, not just frequency counts. The bar chart in Figure 1.16 shows the percent increases in the Dow Jones, Standard and Poor 500 (S & P), and Nasdaq stock indexes from May 24th 2000 to May 24th 2001. Notice that both the S & P and the Nasdaq had “negative increases” which means that they decreased in value. In this bar chart, the Y-axis is not frequency but rather the signed quantity percentage increase.\n\n\n\nFigure 1.16: Percent increase in three stock indexes from May 24th 2000 to May 24th 2001.\n\n\nBar charts are particularly effective for showing change over time. Figure Figure 1.17, for example, shows the percent increase in the Consumer Price Index (CPI) over four three-month periods. The fluctuation in inflation is apparent in the graph.\n\n\n\nFigure 1.17: Percent change in the CPI over time. Each bar represents percent increase for the three months ending at the date indicated.\n\n\nBar charts are often used to compare the means of different experimental conditions. Figure 1.18 shows the mean time it took one of us (DL) to move the mouse to either a small target or a large target. On average, more time was required for small targets than for large ones.\n\n\n\nFigure 1.18: Bar chart showing the means for the two conditions.\n\n\nAlthough bar charts can display means, we do not recommend them for this purpose. Box plots should be used instead since they provide more information than bar charts without taking up more space. For example, a box plot of the mouse-movement data is shown in Figure 1.19. You can see that Figure 1.19 reveals more about the distribution of movement times than does Figure 1.18.\n\n\n\nFigure 1.19: Box plots of times to move the mouse to the small and large targets.\n\n\nThe section on qualitative variables presented earlier in this chapter discussed the use of bar charts for comparing distributions. Some common graphical mistakes were also noted. The earlier discussion applies equally well to the use of bar charts to display quantitative variables."
  },
  {
    "objectID": "Chapter1.html#chapter-1-appendix-calculating-percentiles-under-the-third-definition8",
    "href": "Chapter1.html#chapter-1-appendix-calculating-percentiles-under-the-third-definition8",
    "title": "1  Graphical Tools for Describing One Variable at a Time",
    "section": "1.5 Chapter 1 Appendix: Calculating Percentiles Under the Third Definition[8]",
    "text": "1.5 Chapter 1 Appendix: Calculating Percentiles Under the Third Definition[8]\nLet’s begin with an example. Consider the 25th percentile for the 8 numbers in Table 1.5. Notice the numbers are given ranks ranging from 1 for the lowest number to 8 for the highest number.\n\n\nTable 1.5: Test Scores.\n\n\n\n\n\n\n\n\n\n\nNumber\n\nRank\n\n\n\n\n3\n5\n7\n8\n9\n11\n13\n15\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\nThe first step is to compute the rank (R) of the 25th percentile. This is done using the following formula:\n\\[\nR = P/100 x (N + 1)\n\\]\nwhere P is the desired percentile (25 in this case) and N is the number of numbers (8 in this case). Therefore,\n\\[\nR = 25/100 x (8 + 1) = 9/4 = 2.25.\n\\]\nIf R is an integer, the Pth percentile is the number with rank R. When R is not an integer, we compute the Pth percentile by interpolation as follows:\n\nDefine IR as the integer portion of R (the number to the left of the decimal point). For this example, IR = 2.\nDefine FR as the fractional portion of R. For this example, FR = 0.25.\nFind the scores with Rank IR and with Rank IR + 1. For this example, this means the score with Rank 2 and the score with Rank 3. The scores are 5 and 7.\nInterpolate by multiplying the difference between the scores by FR and add the result to the lower score. For these data, this is (0.25)(7 - 5) + 5 = 5.5.\n\nTherefore, the 25th percentile is 5.5. If we had used the first definition (the smallest score greater than 25% of the scores), the 25th percentile would have been 7. If we had used the second definition (the smallest score greater than or equal to 25% of the scores), the 25th percentile would have been 5.\nFor a second example, consider the 20 quiz scores shown in Table 1.6.\n\n\nTable 1.6: 20 quiz scores.\n\n\n\n\n\n\n\n\n\n\nScore\n\nRank\n\n\n\n\n4\n4\n5\n5\n5\n5\n6\n6\n6\n7\n7\n7\n8\n8\n9\n9\n9\n10\n10\n10\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n\n\nWe will compute the 25th and the 85th percentiles. For the 25th,\n\\[\nR = 25/100 x (20 + 1) = 21/4 = 5.25.\n\\]\n\\[\nIR = 5 \\text{ and } FR = 0.25.\n\\]\nSince the score with a rank of IR (which is 5) and the score with a rank of IR + 1 (which is 6) are both equal to 5, the 25th percentile is 5. In terms of the formula:\n\\[\n\\text{ 25th percentile } = (.25) x (5 - 5) + 5 = 5.\n\\]\nFor the 85th percentile,\n\\[\nR = 85/100 x (20 + 1) = 17.85.\n\\]\n\\[\nIR = 17 \\text{ and } FR = 0.85\n\\]\n\nCaution: FR does not generally equal the percentile to be computed as it does here.\n\nThe score with a rank of 17 is 9 and the score with a rank of 18 is 10. Therefore, the 85th percentile is:\n\\[\n(0.85)(10 - 9) + 9 = 9.85\n\\]\nConsider the 50th percentile of the numbers 2, 3, 5, 9.\n\\[\nR = 50/100 x (4 + 1) = 2.5.\n\\]\n\\[\nIR = 2 \\text{ and } FR = 0.5.\n\\]\nThe score with a rank of IR is 3 and the score with a rank of IR + 1 is 5. Therefore, the 50th percentile is:\n\\[\n(0.5)(5 - 3) + 3 = 4.\n\\]\nFinally, consider the 50th percentile of the numbers 2, 3, 5, 9, 11.\n\\[\nR = 50/100 x (5 + 1) = 3.\n\\]\n\\[\nIR = 3 \\text{ and } FR = 0.\n\\]\nWhenever FR = 0, you simply find the number with rank IR. In this case, the third number is equal to 5, so the 50th percentile is 5. You will also get the right answer if you apply the general formula:\n\\[\n\\text{ 50th percentile }= (0.00) (9 - 5) + 5 = 5.\n\\]\n[1] This section is adapted from Heidi Ziemer. “Variables.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/introduction/variables.html\n[2] This section is adapted from David M. Lane. “Percentiles.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/introduction/percentiles.html\n[3] This section is adapted from David M. Lane. “Graphing Qualitative Variables.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/graphing_distributions/graphing_qualitative.html\n[4] This section is adapted from David M. Lane. “Histograms.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/graphing_distributions/histograms.html\n[5] This section is adapted from David M. Lane. “Box Plots.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/graphing_distributions/boxplots.html\n[6] http://onlinestatbook.com/2/case_studies/stroop.html\n[7] This section is adapted from David M. Lane. “Bar Charts.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/graphing_distributions/bar_chart.html\n[8] This section is adapted from David M. Lane. “Percentiles.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/introduction/percentiles.html"
  },
  {
    "objectID": "Chapter2.html#measures-of-central-tendency1",
    "href": "Chapter2.html#measures-of-central-tendency1",
    "title": "2  Statistics for Describing One Variable at a Time",
    "section": "2.1 Measures of Central Tendency[1]",
    "text": "2.1 Measures of Central Tendency[1]\n\n2.1.1 Mean\nThe mean[2] is the most common measure of central tendency. It is simply the sum of the numbers divided by the number of numbers. When using symbols and formulas to represent different statistics, we often distinguish between whether we are looking at a “sample” or a “population.” We’ll cover this distinction in more detail in our chapter about estimation. For now, think of a pollster who has conducted a survey with a sample of 1000 people. Even though only 1000 people responded to the survey, the pollster is actually interested in estimating the attitudes of a larger population—the entire public.\nThe symbol \\(μ\\) is used for the mean of a population. The symbol \\(M\\) is used for the mean of a sample. The formula for \\(μ\\) is shown below:\n\\[\nμ = ΣX/N\n\\]\nwhere \\(ΣX\\) is the sum of all the numbers in the population and \\(N\\) is the number of numbers in the population.\nThe formula for M is essentially identical:\n\\[\nM = ΣX/N\n\\]\nwhere \\(ΣX\\) is the sum of all the numbers in the sample and \\(N\\) is the number of numbers in the sample.\nAs an example, the mean of the numbers 1, 2, 3, 6, 8 is 20/5 = 4 regardless of whether the numbers constitute the entire population or just a sample from the population.\nTable 2.1 shows the number of touchdown (TD) passes thrown by each of the 31 teams in the National Football League in the 2000 season. The mean number of touchdown passes thrown is 20.4516 as shown below.\n\\[\n\\begin{equation}\n\\begin{split}\nμ = ΣX/N\\\n\\\\\n= 634/31\\\n\\\\\n= 20.4516\n\\end{split}\n\\end{equation}\n\\]\n\n\nTable 2.1: Number of touchdown passes.\n\n\n\n\n\n\n\n\n37 33 33 32 29 28 28 23 22 22 22 21 21 21 20 20 19 19 18 18 18 18 16 15 14 14 14 12 12 9 6\n\n\n\n\n\n\n\n2.1.2 Median\nThe median is also a frequently used measure of central tendency. The median is the midpoint of a distribution: the same number of scores is above the median as below it. For the data in Table 2-1, there are 31 scores. The 16th highest score (which equals 20) is the median because there are 15 scores below the 16th score and 15 scores above the 16th score. The median can also be thought of as the 50th percentile.\n\n2.1.2.1 Computation of the Median\nWhen there is an odd number of numbers, the median is simply the middle number. For example, the median of 2, 4, and 7 is 4. When there is an even number of numbers, the median is the mean of the two middle numbers. Thus, the median of the numbers 2, 4, 7, 12 is (4+7)/2 = 5.5.\n\n\n\n2.1.3 Mode\nThe mode is the most frequently occurring value. For the data in Table 2.1, the mode is 18 since more teams (4) had 18 touchdown passes than any other number of touchdown passes. With continuous data such as response time measured to many decimals, the frequency of each value is one since no two scores will be exactly the same (see discussion of continuous variables). Therefore the mode of continuous data is normally computed from a grouped frequency distribution. Table 2.2 shows a grouped frequency distribution for the target response time data. Since the interval with the highest frequency is 600-700, the mode is the middle of that interval (650).\n\n\nTable 2.2: Grouped frequency distribution.\n\n\n\n\n\n\n\n\n\nRange\nFrequency\n\n\n\n\n500-600\n600-700\n700-800\n800-900\n900-1000\n1000-1100\n3\n6\n5\n5\n0\n1"
  },
  {
    "objectID": "Chapter2.html#comparing-measures-of-central-tendency3",
    "href": "Chapter2.html#comparing-measures-of-central-tendency3",
    "title": "2  Statistics for Describing One Variable at a Time",
    "section": "2.2 Comparing Measures of Central Tendency[3]",
    "text": "2.2 Comparing Measures of Central Tendency[3]\nHow do the various measures of central tendency compare with each other? For symmetric distributions, the mean and median are equal, as is the mode except in bimodal distributions. Differences among the measures occur with skewed distributions. Figure 2.1 shows the distribution of 642 scores on an introductory psychology test. Notice this distribution has a slight positive skew.\n\n\n\nFigure 2.1: A distribution with a positive skew.\n\n\nMeasures of central tendency are shown in Table 2.3. Notice they do not differ greatly, with the exception that the mode is considerably lower than the other measures. When distributions have a positive skew, the mean is typically higher than the median, although it may not be in bimodal distributions. For these data, the mean of 91.58 is higher than the median of 90.\n\n\nTable 2.3: Measures of central tendency for the test scores.\n\n\n\n\n\n\n\n\n\nMeasure\nValue\n\n\n\n\nMode\nMedian\nMean\n84.00\n90.00\n91.58\n\n\n\n\n\nThe distribution of baseball salaries (in 1994) shown in Figure 2.2 has a much more pronounced skew than the distribution in Figure 2.1.\n\n\n\nFigure 2.2: A distribution with a very large positive skew. This histogram shows the salaries of major league baseball players (in thousands of dollars: 25 equals 250,000).\n\n\nTable 2.4 shows the measures of central tendency for these data. The large skew results in very different values for these measures. No single measure of central tendency is sufficient for data such as these. If you were asked the very general question: “So, what do baseball players make?” and answered with the mean of $1,183,000, you would not have told the whole story since only about one third of baseball players make that much. If you answered with the mode of $250,000 or the median of $500,000, you would not be giving any indication that some players make many millions of dollars. Fortunately, there is no need to summarize a distribution with a single number. When the various measures differ, our opinion is that you should report the mean and the median. Sometimes it is worth reporting the mode as well. In the media, the median is usually reported to summarize the center of skewed distributions. You will hear about median salaries and median prices of houses sold, etc. This is better than reporting only the mean, but it would be informative to hear more statistics.\n\n\nTable 2.4: Measures of central tendency for baseball salaries (in thousands of dollars).\n\n\n\n\n\n\n\n\n\n\nMeasure\n\nValue\n\n\n\n\nMode\nMedian\nMean\n\n250\n500\n1,183"
  },
  {
    "objectID": "Chapter2.html#measures-of-spread4",
    "href": "Chapter2.html#measures-of-spread4",
    "title": "2  Statistics for Describing One Variable at a Time",
    "section": "2.3 Measures of Spread[4]",
    "text": "2.3 Measures of Spread[4]\n\n2.3.1 What is Variability?\nVariability refers to how “spread out” a group of scores is. To see what we mean by spread out, consider graphs in ?fig-quizzes. These graphs represent the scores on two quizzes. The mean score for each quiz is 7.0. Despite the equality of means, you can see that the distributions are quite different. Specifically, the scores on Quiz 1 are more densely packed and those on Quiz 2 are more spread out. The differences among students were much greater on Quiz 2 than on Quiz 1.\n \nThe terms variability, spread, and dispersion are synonyms, and refer to how spread out a distribution is. Just as in the section on central tendency where we discussed measures of the center of a distribution of scores, in this chapter we will discuss measures of the variability of a distribution. There are four frequently used measures of variability: the range, interquartile range, variance, and standard deviation. In the next few paragraphs, we will look at each of these four measures of variability in more detail.\n\n\n2.3.2 Range\nThe range is the simplest measure of variability to calculate, and one you have probably encountered many times in your life. The range is simply the highest score minus the lowest score. Let’s take a few examples. What is the range of the following group of numbers: 10, 2, 5, 6, 7, 3, 4? Well, the highest number is 10, and the lowest number is 2, so 10 - 2 = 8. The range is 8. Let’s take another example. Here’s a dataset with 10 numbers: 99, 45, 23, 67, 45, 91, 82, 78, 62, 51. What is the range? The highest number is 99 and the lowest number is 23, so 99 - 23 equals 76; the range is 76. Now consider the two quizzes shown in Figure 2-3. On Quiz 1, the lowest score is 5 and the highest score is 9. Therefore, the range is 4. The range on Quiz 2 was larger: the lowest score was 4 and the highest score was 10. Therefore the range is 6.\n\n\n2.3.3 Interquartile Range\nThe interquartile range (IQR) is the range of the middle 50% of the scores in a distribution. It is computed as follows:\n\\[\nIQR = \\text{ 75th percentile } - \\text{ 25th percentile }\n\\]\nFor Quiz 1, the 75th percentile is 8 and the 25th percentile is 6. The interquartile range is therefore 2. For Quiz 2, which has greater spread, the 75th percentile is 9, the 25th percentile is 5, and the interquartile range is 4. Recall that in the discussion of box plots, the 75th percentile was called the upper hinge and the 25th percentile was called the lower hinge. Using this terminology, the interquartile range is referred to as the H-spread.\nA related measure of variability is called the semi-interquartile range. The semi-interquartile range is defined simply as the interquartile range divided by 2. If a distribution is symmetric, the median plus or minus the semi-interquartile range contains half the scores in the distribution.\n\n\n2.3.4 Variance\nVariability can also be defined in terms of how close the scores in the distribution are to the middle of the distribution. Using the mean as the measure of the middle of the distribution, the variance is defined as the average squared difference of the scores from the mean. The data from Quiz 1 are shown in Table 2.5. The mean score is 7.0. Therefore, the column “Deviation from Mean” contains the score minus 7. The column “Squared Deviation” is simply the previous column squared.\n\n\nTable 2.5: Calculation of Variance for Quiz 1 scores.\n\n\n\n\n\n\n\n\n\n\n\n\nScores\n\nDeviation from Mean\n\nSquared Deviation\n\n\n\n\n9\n\n2\n\n4\n\n\n\n\n9\n\n2\n\n4\n\n\n\n\n9\n\n2\n\n4\n\n\n\n\n8\n\n1\n\n1\n\n\n\n\n8\n\n1\n\n1\n\n\n\n\n8\n\n1\n\n1\n\n\n\n\n8\n\n1\n\n1\n\n\n\n\n7\n\n0\n\n0\n\n\n\n\n7\n\n0\n\n0\n\n\n\n\n7\n\n0\n\n0\n\n\n\n\n7\n\n0\n\n0\n\n\n\n\n7\n\n0\n\n0\n\n\n\n\n6\n\n-1\n\n1\n\n\n\n\n6\n\n-1\n\n1\n\n\n\n\n6\n\n-1\n\n1\n\n\n\n\n6\n\n-1\n\n1\n\n\n\n\n6\n\n-1\n\n1\n\n\n\n\n6\n\n-1\n\n1\n\n\n\n\n5\n\n-2\n\n4\n\n\n\n\n5\n\n-2\n\n4\n\n\n\n\n\n\nMeans\n\n\n\n\n\n\n7\n\n0\n\n1.5\n\n\n\n\n\nOne thing that is important to notice is that the mean deviation from the mean is 0. This will always be the case. The mean of the squared deviations is 1.5. Therefore, the variance is 1.5. Analogous calculations with Quiz 2 show that its variance is 6.7. The formula for the variance is:\n\\[\nσ^2=\\frac{Σ(X-µ)^2}{N}\n\\]\nwhere \\(σ2\\) is the variance, \\(μ\\) is the mean, and \\(N\\) is the number of numbers. For Quiz 1, \\(μ\\) = 7 and \\(N\\) = 20.\nIf the variance in a sample is used to estimate the variance in a population, then the previous formula underestimates the variance and the following formula should be used:\n\\[\ns^2=\\frac{Σ(X-M)^2}{N-1}\n\\]\nwhere \\(s2\\) is the estimate of the variance and \\(M\\) is the sample mean.\nNote that \\(M\\) is the mean of a sample taken from a population with a mean of \\(μ\\). Since, in practice, the variance is usually computed in a sample, this formula is most often used. The simulation “estimating variance” illustrates the bias in the formula with \\(N\\) in the denominator.\nLet’s take a concrete example. Assume the scores 1, 2, 4, and 5 were sampled from a larger population. To estimate the variance in the population you would compute \\(s^2^\\) as follows:\n\\[\nM = (1 + 2 + 4 + 5)/4 = 12/4 = 3\n\\]\n\\[\ns^2 = [(1-3)^2 + (2-3)^2 + (4-3)^2 + (5-3)^2]/(4-1)\n\\\\\n= (4 + 1 + 1 + 4)/3 = 10/3 = 3.333\n\\]\n\n\n2.3.5 Standard Deviation\nThe standard deviation is simply the square root of the variance. This makes the standard deviations of the two quiz distributions 1.225 and 2.588. We can interpret the standard deviation of X as approximating the typical distance between a given value of X and the mean of X. For example, suppose I tell you about a prison where the prisoners have a mean age of 42 years with a standard deviation of 8 years. If I randomly select one prisoner and ask you to guess their age, you should probably guess 42 since I’ve told you that is the mean. But even though 42 is your best guess, you can expect your guess to be off by about 8 years since the standard deviation is 8 (meaning the typical distance between a random prisoner’s age and the mean age is approximately 8)."
  },
  {
    "objectID": "Chapter2.html#transforming-variables5",
    "href": "Chapter2.html#transforming-variables5",
    "title": "2  Statistics for Describing One Variable at a Time",
    "section": "2.4 Transforming Variables[5]",
    "text": "2.4 Transforming Variables[5]\nOften it is necessary to transform data from one measurement scale to another. For example, you might want to convert height measured in feet to height measured in inches. Table 2.6 shows the heights of four people measured in both feet and inches. To transform feet to inches, you simply multiply by 12. Similarly, to transform inches to feet, you divide by 12.\n\n\nTable 2.6: Converting between feet and inches.\n\n\n\n\n\n\n\n\n\n\nFeet\n\nInches\n\n\n\n\n5.00\n6.25\n5.50\n5.75\n\n60\n75\n66\n69\n\n\n\n\n\nSome conversions require that you multiply by a number and then add a second number. A good example of this is the transformation between degrees Centigrade and degrees Fahrenheit. Table 2.7 shows the temperatures of 5 US cities in the early afternoon of November 16, 2002.\n\n\nTable 2.7: Temperatures in 5 cities on 11/16/2002.\n\n\n\n\n\n\n\n\n\n\n\n\nCity\n\nDegrees Fahrenheit\n\nDegrees Centigrade\n\n\n\n\nHouston\nChicago\nMinneapolis\nMiami\nPhoenix\n\n54\n37\n31\n78\n70\n\n12.22\n2.78\n-0.56\n25.56\n21.11\n\n\n\n\n\nThe formula to transform Centigrade to Fahrenheit is:\n\\[\nF = 1.8C + 32\n\\]\nThe formula for converting from Fahrenheit to Centigrade is\n\\[\nC = 0.5556F - 17.778\n\\]\nThe transformation consists of multiplying by a constant and then adding a second constant. For the conversion from Centigrade to Fahrenheit, the first constant is 1.8 and the second is 32.\nFigure 2.3 shows a plot of degrees Centigrade as a function of degrees Fahrenheit. Notice that the points form a straight line. This will always be the case if the transformation from one scale to another consists of multiplying by one constant and then adding a second constant. Such transformations are therefore called linear transformations.\n\n\n\nFigure 2.3: Degrees Centigrade as a function of degrees Fahrenheit.\n\n\nSo far, we’ve discussed transformations that are probably familiar to you. A type of transformation that may be new to you is standardization or creating Z scores. A value from any distribution can be transformed into a Z score using the following formula:\n\\[\nZ = \\frac{(X - μ)}{σ}\n\\]\nwhere \\(Z\\) is the new value, \\(X\\) is the value on the original distribution, \\(μ\\) is the mean of the original distribution, and \\(σ\\) is the standard deviation of the original distribution.\nAs a simple application, suppose you want the Z score for a value of 26 taken from a distribution with a mean of 50 and a standard deviation of 10. Applying the formula, we obtain:\n\\[\nZ = (26 - 50)/10 = -2.4\n\\]\nIf all the values in a distribution are transformed to Z scores, then the new distribution will have a mean of 0 and a standard deviation of 1. This process of transforming a distribution to one with a mean of 0 and a standard deviation of 1 is called standardizing the distribution. Sometimes it will be easier to work with a standardized version of a variable.\n\n2.4.1 Log Transformations[6]\nSometimes it is also useful to use transformations that are not linear. For example, the log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.\nFigure 2.4 shows an example of how a log transformation can make patterns more visible. Both graphs plot the brain weight of animals as a function of their body weight. The raw weights are shown in the upper panel; the log-transformed weights are plotted in the lower panel.\n\n\n\nFigure 2.4: Scatter plots of brain weight as a function of body weight in terms of both raw data (upper panel) and log-transformed data (lower panel).\n\n\nIt is hard to discern a pattern in the upper panel whereas the strong relationship is shown clearly in the lower panel.\n[1] This section is adapted from David M. Lane. “Measures of Central Tendency.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/summarizing_distributions/measures.html\n[2] More specifically, the arithmetic mean is the most common measure of central tendency. Although the arithmetic mean is not the only “mean” (there is also a geometric mean), it is by far the most commonly used. Therefore, if the term “mean” is used without specifying whether it is the arithmetic mean, the geometric mean, or some other mean, it is assumed to refer to the arithmetic mean.\n[3] This section is adapted from David M. Lane. “Comparing Measures of Central Tendency.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/summarizing_distributions/comparing_measures.html\n[4] This section is adapted from David M. Lane. “Measures of Variability.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/summarizing_distributions/variability.html\n[5] The initial part of this section is adapted from David M. Lane. “Linear Transformations.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/introduction/linear_transforms.html. There is also material adapted from David M. Lane. “Standard Normal Distribution.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/normal_distribution/standard_normal.html.\n[6] This subsection is adapted from David M. Lane. “Log Transformations.” Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com/2/transformations/log.html"
  },
  {
    "objectID": "Chapter11.html#assumptions-about-error-terms",
    "href": "Chapter11.html#assumptions-about-error-terms",
    "title": "3  Models and Uncertainty",
    "section": "3.1 Assumptions about error terms",
    "text": "3.1 Assumptions about error terms\nIt’s easy to write out an equation that includes an error term, but we are not going to be able to do much with our model unless we make some assumptions about the error term. One of the most important (and challenging) parts of doing statistical analysis is making assumptions about the possible values of the error term. Different assumptions about the error term can result in very different conclusions.\nLet’s again consider the simple model of happiness that was introduced above:\n\\[\nhappiness=3.0+2.3×income+ε\n\\]\nWe might assume the following things about the error term (ε):\n\nThe values of the error term (ε) can be described by a normal distribution with a mean of 0\nKnowing someone’s income doesn’t help us predict the values of the error term (ε)\n\nWhat do these two assumptions mean?\nFirst, if the error term (ε) follows a normal distribution with a mean of zero, that means that (according to our model), people are just as likely to have a positive value of the error term as they are to have a negative value of the error term. In other words, all those factors we haven’t accounted for in our model are equally likely to push people in the direction of being happier or in the direction of being less happy. Our model and assumptions tell us that if we predict happiness purely based on income, we’ll overestimate some people’s happiness, and we’ll underestimate an equal number of people’s happiness.1\nSecond, these assumptions allow us to describe how much individual observations will tend to deviate from our income-based predictions. We haven’t specified in our assumptions what the standard deviation is for the normal distribution for the error term (ε), but statistical analysis will let us estimate the standard deviation of an error term. And we know that there is a 95% chance of drawing a value within two standard deviations of the mean for any normal distribution. So whatever the standard deviation of the error term (ε) is, we would expect that 95% of the time, the error term will take on a value that is within two standard deviations of zero. Conversely, 5% of the time, the error term will take on a value that is more than two standard deviations from zero. Suppose that the standard deviation of the error term (ε) happens to be three. If we have a dataset containing the income and happiness of 1,000 randomly selected people, we would expect that about 950 of these people will have a level of happiness that falls within six units of our income-based prediction. But for about 50 of these people, our prediction of their happiness will be off by more than six units.\nThird, our assumptions imply that income is not tied in any consistent way to (the total sum of) factors other than income that also affect peoples’ happiness. Remember, the error term (ε) represents all factors other than income that affect satisfaction. If income is related to these other factors, then the value of income should help us predict the value of the error term. For example, if having a stable environment in childhood tends to cause both higher incomes and greater happiness in adulthood, the error term will partially reflect the effect of childhood stability on happiness, so high incomes (which are partially caused by childhood stability) will be probably be predictive of a more positive error term. This would constitute a violation of our assumptions since we specifically indicated that income wasn’t predictive of the error term. As this example illustrates, our assumptions about error terms are often quite strict, making it rather difficult in practice to build good models that account for uncertainty."
  },
  {
    "objectID": "Chapter11.html#models-and-probabilistic-thinking",
    "href": "Chapter11.html#models-and-probabilistic-thinking",
    "title": "3  Models and Uncertainty",
    "section": "3.2 Models and probabilistic thinking",
    "text": "3.2 Models and probabilistic thinking\nDespite the difficulty inherent in building models that accommodate uncertainty, we have little alternative unless we wish to only build models of things we think we can predict with 100% accuracy. And fortunately, our models do not always have to be perfectly correct in order to generate useful predictions or explanations. As the statistician George Box famously said, “all models are wrong, but some are useful.”\nAn important part of learning to do good statistical analysis is learning to think clearly about models so that you can pick out a model that is useful for whatever it is you want to accomplish. And the first step toward understanding many statistical models is learning to think about the world in probabilistic terms, as we’ve done here in this reading. Probabilistic thinking asks questions like:\n\nBased on what I do know and what I don’t know, what can I predict?\nHow does adding or removing different pieces of information change my prediction?\nHow much uncertainty is there in my prediction?\nHow often will my prediction differ greatly from what actually happens (even if my model is correct)?"
  },
  {
    "objectID": "Chapter11.html#footnotes",
    "href": "Chapter11.html#footnotes",
    "title": "3  Models and Uncertainty",
    "section": "",
    "text": "Note that these deviations from our prediction don’t imply that our model is wrong; our model explicitly acknowledges that we’ll get only imperfect estimates if we predict happiness based on income, since the unobserved error term (ε) also contributes to happiness.↩︎"
  },
  {
    "objectID": "Chapter12.html#predicting-extraversion-using-gender",
    "href": "Chapter12.html#predicting-extraversion-using-gender",
    "title": "4  Regression with Qualitative Independent Variables",
    "section": "4.1 Predicting extraversion using gender",
    "text": "4.1 Predicting extraversion using gender\nIf I want to describe differences in extraversion by gender in this dataset, I can compute the mean value of extraversion for males and for females. It turns out that males have an average extraversion of -0.46 while females’ average level of extraversion is 0.53. Thus, the average female is about 1-point more extraverted than the average male. But of course, there is lots of variation in extraversion among both groups:\n\nThere are plenty of females who are introverts and plenty of males who are extroverts.\nIf you asked me to guess the extroversion level of someone and the only thing you told me about them was their gender, my best bet would probably be to guess the average extroversion level for someone of that gender. So for a female I knew nothing else about, I would guess their extroversion to be 0.53, while for a male I’d guess -0.46.\nSocial scientists use the dependent variable to describe the variable they’re making a prediction about and independent variable to describe the variables that help them make that prediction. So in this example, extraversion is my dependent variable and gender is my independent variable.\nWhen we’re working with data, sometimes it’s helpful to express how I would make a guess about a dependent variable (extraversion) based on other factors (gender) using a mathematical formula. In fact, this is exactly what we do when we run a regression. There are many ways I could write this formula, but I’ll show just two for now. First, I could write:\n\\[\n\\widehat{Extraversion}=0.53×Female-0.46×Male\n\\tag{4.1}\\]\nNotice I’ve added a “hat” above the name of the variable Extraversion; this hat means that I’m making a guess about the value of that variable (I’m guessing the level of extraversion based on gender). The equation has two other variables Female and Male, and these two variables will take on a value of 1 if the person’s gender is equal to the name of the variable and will otherwise take on a value of 0. For a female, Female will equal 1 and Male will equal 0, giving us:\n\\[\n\\widehat{Extraversion}=0.53×(1)-0.46×(0)=0.53\n\\]\nSo our guess for the level of extroversion (\\(\\widehat{Extraversion}\\)) of a female we know nothing about is 0.53.\nFor a male, our guess is:\n\\[\n\\widehat{Extraversion}=0.53×(0)-0.46×(1)=-0.46\n\\]\nThere’s a second way I can write my formula, which will turn out to be more useful in the future when we come to consider multiple factors at the same time that might help us predict the value of a dependent variable. Rather than having two variables to represent gender in my equation, I can just use one:\n\\[\n\\widehat{Extraversion}=0.53-0.99×Male\n\\tag{4.2}\\]\nIn Equation 4.2, we start from female as our baseline. Notice that the first number we see (0.53) is our guess for the value of extraversion for a female. When we’re considering a female, Male=0, so:\n\\[\n\\widehat{Extraversion}=0.53-0.99×0=0.53\n\\]\nThus, we get the right prediction for females from this equation, even though we didn’t include a variable specifically for females. If we have a male, Male=1, so we get:\n\\[\n\\widehat{Extraversion}=0.53-0.99×1=-0.46\n\\]\nThis is the same prediction we got before. Remember, I decided to initially just analyze respondents who selected either male or female. Since we are only considering two categories (male or female), and each respondent is either a male or a female, saying Male = 1 lets me know that Female = 0. It’s actually repetitive in this context to both say that Male = 1 and Female = 0. Similarly, saying Male = 0 implies that Female = 1. So I can simplify my equation by just including one variable to indicate binary gender.\nNotice that in Equation 4.2, the number next to Male is equal to the difference between the average level of extraversion for females and the average level for males (0.53-(-0.46)=0.99). This is because Equation 4.2 starts with females as the baseline, so to get our prediction for males, we have to adjust our baseline prediction by the average difference for males.\nEquation 4.2 is also typically how we will arrange our equation when we’re running a regression."
  },
  {
    "objectID": "Chapter12.html#prediction-with-more-than-two-categories-for-gender",
    "href": "Chapter12.html#prediction-with-more-than-two-categories-for-gender",
    "title": "4  Regression with Qualitative Independent Variables",
    "section": "4.2 Prediction with more than two categories for gender",
    "text": "4.2 Prediction with more than two categories for gender\nI now move beyond the gender binary and consider the “other” category in survey responses. I’ll refer to this other category as “non-binary” gender. The average level of extraversion among those with non-binary gender is -5.66. So non-binary people tend to be quite a bit more introverted than those who identify as male or female. As with males and females, there is considerable variation among non-binary people:\n\nThe number of non-binary respondents is relatively small (102), so it’s not terrible surprising that this histogram looks a bit choppier than the ones we saw before.\nAgain, if we had to make a guess about the level of extraversion of someone, and all we knew about that person was that their gender was non-binary, we would probably want to guess the mean value among non-binary respondents (-5.66). Modifying Equation 4.1 to incorporate a third category is relatively straightforward:\n\\[\n\\widehat{Extraversion}=0.53×Female-0.46×Male-5.69×Other\n\\tag{4.3}\\]\nFor someone who identifies as female, we would plug in Female = 1, Male = 0, and Other = 0:\n\\[\n\\widehat{Extraversion}=0.53×(1)-0.46×(0)-5.66×(0)=0.53\n\\]\nIf someone identifies as non-binary, we would use Female = 0, Male = 0, and Other = 1:\n\\[\n\\widehat{Extraversion}=0.53×(0)-0.46×(0)-5.66×(1)=-5.66\n\\]\nWe can also return to the format of Equation 4.2 but modify it to include the other category. This is how we will typically write our equation if we are doing a regression:\n\\[\n\\widehat{Extraversion}=0.53-0.99×Male-6.19×Other\n\\tag{4.4}\\]\nNow that there are three possible values for gender (female, male, and other), knowing the value of Male doesn’t necessarily allow us to conclude what the vale of female is. If , the individual could identify as either female or non-binary. So we have to include a second variable. In this case, we chose to include the variable Other. If we know the values of Male and Other, we can always figure out the value of Female by process of elimination.\nFor a non-binary person, we plug in Male = 0, and Other = 1:\n\\[\n\\widehat{Extraversion}=0.53-0.99×(0)-6.19×(1)=-5.66\n\\]\nWhen considering a female, we use Male = 0, and Other = 0:\n\\[\n\\widehat{Extraversion}=0.53-0.99×(0)-6.19×(0)=0.53\n\\]\nEquation 4.3 and Equation 4.4 communicate an equivalent method of making a prediction about extraversion based on gender; they just offer this information in two different formats. Equation 4.4 might be a bit trickier to understand for now, but it will become very useful in the future.\nNotice that we can talk about gender either as one qualitative variable with three possible values (female, male, or other), or we can talk about it as a series of three dummy variables (Female, Male, and Other) that can take each on a value of either 0 or 1. This can make things a bit confusing, but the important thing to remember is that when we have a qualitative variable with more than two categories, we’ll need to break out the categories into a set of dummy variables for purposes of representing the qualitative variable in an equation.\nHowever, as Equation 4.2 and Equation 4.4 illustrate, we don’t necessarily need a dummy variable for every single category. Specifically, whenever we want to create an equation with a qualitative independent variable in a format like Equation 4.2 or Equation 4.4, the number of dummy variables should be equal to the number of categories minus one. Since our gender variable can take on three possible values in this example, we included two independent variables in Equation 4.4. No dummy variable is included for female, so we call female the omitted category or the baseline category. Remember, the first number in Equation 4.4 is 0.53, which represents our guess for females—the baseline category. If we instead had a qualitative variable with five categories, we would include four dummy variables in our equation."
  },
  {
    "objectID": "Chapter12.html#footnotes",
    "href": "Chapter12.html#footnotes",
    "title": "4  Regression with Qualitative Independent Variables",
    "section": "",
    "text": "https://openpsychometrics.org/_rawdata/ (the file I used is called “BIG5.zip”)↩︎"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Statistics Minus The Math: An Introduction for the Social Sciences",
    "section": "Introduction",
    "text": "Introduction\nThis version (1.2) was updated 1/28/2023.\nVersion 1.1 available at https://nathanfavero.com/teaching-tools/. The only change from 1.1 is that the discussion of transforming variables now appears in Ch. 2 (rather than Ch. 3).\nThis book was largely adapted from the public domain resource Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/ Project Leader: David M. Lane, Rice University)."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Statistics Minus The Math: An Introduction for the Social Sciences",
    "section": "Contents",
    "text": "Contents"
  }
]